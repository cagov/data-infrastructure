{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CalData Data Services and Engineering Infrastructure","text":"<p>This is the technical documentation for CalData's Data Services and Engineering (DSE) projects. It consists of processes, conventions, instructions, and architecture diagrams.</p>"},{"location":"code/azdevops-project-management/","title":"Azure DevOps and Project Management","text":"<p>While GitHub is the primary tool used by the Data Operations team for code development and task tracking, we sometimes use Azure DevOps instead for clients that are already established in the Azure environment. This page covers practices and guidelines to be followed when using DevOps from a project management perspective to ensure work items contain sufficient detail so we know what needs to be done, how much effort is involved, and how to prioritize it against other issues.</p>"},{"location":"code/azdevops-project-management/#work-item-types","title":"Work Item Types","text":"<ol> <li>Epics are used to group work under larger categories, for example \"Phase 1 Data Source Modeling\".<ol> <li>All work should roll up to an Epic. These can be broad categories like \"Phase 1 Data Modeling\", for example.</li> </ol> </li> <li>Issues and tasks are used to track work.<ol> <li>Issues will be our primary work item type.</li> <li>Tasks to break the work out further as needed. These are the detailed steps needed to complete the issue. Tasks can also be used to assign work that is split between multiple people, since DevOps only allows for one assignee on a work item.</li> </ol> </li> </ol>"},{"location":"code/azdevops-project-management/#work-item-guidelines","title":"Work Item Guidelines","text":"<ol> <li>Title: This can be short, but should give enough detail to identify what work this issue is related to and differentiate it from similar task. For example, \u201cReview Documentation for [Specific Data Source Name]\u201d, as opposed to \u201cReview Documentation\u201d which is too vague.</li> <li>Description: This should be detailed enough that anyone viewing the issue knows what work needs to be done, and what the ideal outcome or deliverable is once this work is complete (success criteria).</li> <li>The following items should always be filled in when creating a new issue. If you are unsure what to enter, reach out to the Project Manager to discuss, or bring it up in daily standup or weekly sprint planning meetings to discuss with the team.<ol> <li>Assignee: If you are unsure who to assign the item to, leave it blank for now. Then add a comment that you think it may need to be assigned to someone else and we can discuss in the next team meeting. Note that unlike GitHub, Azure DevOps only allows assigning a work item to a single person. To assign something to multiple people it must be broken out into multiple work items.</li> <li>Priority: Indicates how soon the work needs to start. See definitions in the appendix at the end of this document.</li> <li>Effort: Effort indicates how much time the work will take to complete once it has started. See definitions in the appendix at the end of this document. Tasks should ideally be sized appropriately so that they can be completed in &lt;1 sprint of work. So Tiny (1 hour), Small (1 day), and Medium (1 week) are appropriate sizes for tasks. A Large (2+ weeks) or X-Large (months) task should be broken up into smaller work items for easier tracking from sprint to sprint.</li> <li>Iteration (aka Sprint): We track work in two-week sprint cycles. Sprint names follow the a convention of [current year]-[sequential sprint number]. So the first sprint of the year 2025 is \"2025-01\". The last sprint of the year 2025 is \"2025-26\".<ol> <li>If work on an issue or task will start within the next 2 sprints, assign it to the appropriate sprint during Sprint Planning meetings. Work planned further than 2 sprints out does not need to be assigned to a sprint and can be left in the backlog, unless there is a known due date or we want to make sure something starts at a certain date.</li> </ol> </li> <li>Area: This is a high level category at a broader level than Epic. Examples are Data Modeling, Training, Data Onboarding, etc. This is used for tracking purposes only. The Project Manager can help assign to the correct Area if needed.</li> <li>Status: Assign a status on the current state of the issue. See below for status options. Make sure to fully close a work item when completed by setting the status to Done.</li> </ol> </li> </ol>"},{"location":"code/azdevops-project-management/#effort-definitions","title":"Effort Definitions","text":"<p>Note: Effort is a numeric open text box in Azure DevOps, rather than a drop-down with predetermined options like in GitHub. Below we list the numeric value to enter for the different Effort levels, in order to standardize across our projects. The sizes we use are roughly equivalent to the number of days we think the work will take to complete. We use 0.125 for Tiny as it aligns with one hour in a 8-hour work day.</p> <p>Tiny = 0.125</p> <ul> <li>Easy task, something that can be completed in about an hour. It may not require issue creation but could use as a reminder to complete the task later in the sprint (if something takes less than 30 minutes, no need for an issue).</li> <li>Example: Sending a follow-up email</li> </ul> <p>Small = 1</p> <ul> <li>A task that can be completed in one day</li> <li>Example: Reviewing documentation on an API to determine how to onboard a new data source.</li> </ul> <p>Medium = 5</p> <ul> <li>A task that can be completed in about a week</li> <li>More complex but still straightforward to complete</li> <li>Example: Creating a staging or intermediate data model for a source that is well-understood and does not require complex logic.</li> </ul> <p>Large = 10</p> <ul> <li>A task that can be completed in 2 or more weeks</li> <li>Increased complexity, may require slower manual work or unknowns that require research and discovery</li> <li>Example: Writing an API script for a new, unknown data source.</li> </ul> <p>X-Large = 20</p> <ul> <li>Tasks that may take a month or longer to complete</li> <li>Consider breaking up into smaller work items if possible</li> <li>Example: Building a full set of end-to-end models for a new data source, including staging, intermediate, and mart models.</li> </ul>"},{"location":"code/azdevops-project-management/#priority-definitions","title":"Priority Definitions","text":"<p>Note: Priority in Azure DevOps is a dropdown with options from 1-4. We use the following definitions for those numbers to standardize across our projects.</p> <p>Urgent = 1</p> <ul> <li>Top priority, usually reserved for unexpected emergency item.</li> <li>Putting everything else on backburner, aiming for completion ASAP</li> <li>Not assigned as part of regular sprint planning process</li> </ul> <p>High = 2</p> <ul> <li>Goal is to complete this sprint</li> </ul> <p>Medium = 3</p> <ul> <li>Less important than High, aim to complete within 2-4 sprints</li> <li>OK to push to next sprint if needed</li> </ul> <p>Low = 4</p> <ul> <li>Work on as time allows, may leave in backlog</li> </ul>"},{"location":"code/azdevops-project-management/#status-definitions","title":"Status Definitions","text":"<ol> <li>To Do: This is the default status when a work item is created, indicates no work has started yet.</li> <li>Doing: Choose this status once work has started on the issue.</li> <li>Done: Indicates the work item is completed. If the work item is linked to a Pull Request, you will be given an option to close it when merging the PR. Or you can choose this status when viewing the work item.</li> </ol>"},{"location":"code/code-review/","title":"Conventions for Code Review and GitHub","text":"<p>This page documents the Data Services and Engineering Team's practices around GitHub-based development and code review.</p>"},{"location":"code/code-review/#creating-a-pull-request","title":"Creating a Pull Request","text":"<p>The process for GitHub-based development is:</p> <ol> <li>Create a new branch from <code>main</code>:     <pre><code>git switch -c &lt;branch-name&gt;\n</code></pre></li> <li>Develop in your branch. Try to keep commits to single ideas. A branch for a good pull request can tell a story.</li> <li>When your branch is ready for review, push it to GitHub:     <pre><code>git push &lt;remote-name&gt; &lt;branch-name&gt;\n</code></pre></li> <li>From the GitHub UI, open a pull request for merging your code into <code>main</code>.</li> <li>Request one or more reviewers.</li> <li>Go through one or several rounds of review, making changes to your branch as necessary. A healthy review is a conversation, and it's normal to have disagreements.</li> <li>When the reviewer is happy, they can approve and merge the pull request!</li> <li>In general, the author of a PR should not approve and merge their own pull request.</li> <li>Delete your feature branch, it's in <code>main</code> now.</li> </ol>"},{"location":"code/code-review/#considerations-when-authoring-a-pull-request","title":"Considerations when authoring a pull request","text":""},{"location":"code/code-review/#have-empathy","title":"Have Empathy","text":"<p>We recommend reading \"Have empathy on pull requests\" and \"How about code reviews?\" as nice references for how to be empathetic as the opener of a pull request.</p> <p>In particular, it's important to remember that you are the subject matter expert for a PR. The reviewer will likely not know anything about the path you took to a particular solution, what approaches did not work, and what tradeoffs you encountered. It's your job to communicate that context for reviewers to help them review your code. This can include comments in the GitHub UI, comments in the code base, and even self-reviews.</p>"},{"location":"code/code-review/#use-linters-and-formatters","title":"Use linters and formatters","text":"<p>Making use of code linters and formatters helps to establish a consistent style for a project and removes a whole whole class of common errors and disagreements. Even if one can take issue with specific conventions or rules, having them used consistently within a team pays big dividends over time.</p> <p>Many of our projects use <code>pre-commit</code> to enforce the linter and formatter conventions. To set up your pre-commit environment locally (requires a Python development environment, run</p> <pre><code>pre-commit install\n</code></pre> <p>The next time you make a commit, the pre-commit hooks will run on the contents of your commit (the first time may be a bit slow as there is some additional setup).</p>"},{"location":"code/code-review/#try-to-avoid-excessive-merge-commits","title":"Try to avoid excessive merge commits","text":"<p>More merge commits in a PR can make review more difficult, as contents from unrelated work can appear in the code diff. Sometimes they are necessary for particularly large or long-running branches, but for most work you should try to avoid them. The following guidelines can help:</p> <ul> <li>Usually branch from the latest <code>main</code></li> <li>Keep feature branches small and focused on a single problem. It is often helpful for both authors and reviewers to have larger efforts broken up into smaller tasks.</li> <li>In some circumstances, a <code>git rebase</code> can help keep a feature branch easy to review and reason about.</li> </ul>"},{"location":"code/code-review/#write-documentation","title":"Write Documentation","text":"<p>If your pull request adds any new features or changes any workflows for users of the project, you should include documentation. Otherwise, the hard work you did to implement a feature may go unnoticed/unused! What this looks like will vary from project to project, but might include:</p> <ul> <li>Sections in a README</li> <li>Markdown/rst documents in the repository</li> <li>Table metadata in a dbt project.</li> </ul>"},{"location":"code/code-review/#write-tests","title":"Write Tests","text":"<p>New functionality ideally should have automated tests. As with documentation, these tests will look different depending upon the project needs. A good test will:</p> <ol> <li>Be separated from production environments</li> <li>If fixing a bug, should actually fix the issue.</li> <li>Guard against regressions</li> <li>Not take so long as to be annoying to run.</li> <li>Not rely on internal implementation details of a project (i.e. use public contracts)</li> </ol> <p>One nice strategy for bugfixes is to write a failing test before making a fix, then verifying that the fix makes the test pass. It is surprisingly common for tests to accidentally not cover the behavior they are intended to.</p>"},{"location":"code/code-review/#reviewing-a-pull-request","title":"Reviewing a pull request","text":""},{"location":"code/code-review/#have-empathy_1","title":"Have Empathy","text":"<p>As above, reviewers should have empathy for the author of a pull request. You as a reviewer are unaware of the constraints and tradeoffs that an author might have encountered.</p> <p>Some general tips for conducting productive reviews:</p> <ul> <li>If there is something you might have done differently, come in with a constructive attitude and try to understand why the author took their approach.</li> <li>Keep your reviews timely (ideally provide feedback within 24 hours)</li> <li>Try to avoid letting a PR review stretch on too long. A branch with many review cycles stretching for weeks is demoralizing to code authors.</li> <li>Remember that perfect is the enemy of the good. A PR that makes an improvement or is a concrete step forward can be merged without having to solve everything. It's perfectly reasonable to open up issues to capture follow-up work from a PR.</li> </ul>"},{"location":"code/code-review/#ci-should-pass","title":"CI should pass","text":"<p>Before merging a pull request, maintainers should make every effort to ensure that CI passes. Often this will require looking into the logs of a failed run to see what went wrong, and alerting the pull request author. Ideally, no pull request should be merged if there are CI failures, as broken CI in main can easily mask problems with other PRs, and a consistently broken CI can be demoralizing for maintainers.</p> <p>However, in practice, there are occasionally flaky tests, broken upstream dependencies, and failures that are otherwise obviously not related to the PR at hand. If that is the case, a reviewer may merge a PR with failing tests, but they should be prepared to follow up with any failures that result from such an unsafe operation.</p> <p>Note: these conventions and recommendations are partially drawn from maintainer guidelines for JupyterHub and Dask.</p>"},{"location":"code/codespaces/","title":"Developing using Codespaces","text":"<p>GitHub Codespaces allow you to spin up an ephemeral development environment in VS Code which includes a git repository, configurations, and pre-installed libraries. It provides an easy way for developers to get started working in a repository, especially if they are uncomfortable</p>"},{"location":"code/codespaces/#creating-a-codespace","title":"Creating a Codespace","text":"<p>Go to the \"Code\" dropdown from the main repository page, select the three dot dropdown, and select \"New with options...\" This will allow more configuration than the default codespace.</p> <p></p> <p>In the codespace configuration form, you will have an option to add \"Recommended Secrets\". This is where you can add your personal Snowflake credentials to your codespace, allowing for development against our Snowflake warehouse, including using dbt. You should only add credentials for accounts that are protected by multi-factor authentication (MFA).</p> <p></p> <p>After you have added your secrets, click \"Create Codespace\". Building it may take a few minutes, but then you should be redirected to a VS Code environment in your browser.</p>"},{"location":"code/codespaces/#launching-an-existing-codespace","title":"Launching an existing Codespace","text":"<p>Once your codespace is created, you should be able to launch it without re-creating it every time using the \"Code\" dropdown, going to \"Open in...\", and selecting \"Open in browser\":</p> <p></p>"},{"location":"code/codespaces/#using-a-codespace","title":"Using a Codespace","text":"<p>Once you have created and configured a codespace, you have access to a relatively full-featured VS Code-based development environment. This includes:</p> <ul> <li>An integrated bash terminal</li> <li>dbt profiles configured</li> <li>All Python dependencies for the project</li> <li>All pre-commit hooks installed</li> <li>git configured using your GitHub account</li> </ul>"},{"location":"code/codespaces/#usage-notes","title":"Usage notes","text":"<ol> <li>When you launch a new codespace, it can take a couple of minutes for all of the extensions to install. In particular, this means that the Python environment may not be fully set-up when you land in VS Code. We recommend closing existing terminal sessions and starting a new one once the extensions have finished installing.</li> <li>The first time you make a commit, the pre-commit hooks will be installed. This may take a few minutes. Subsequent commits will take less time.</li> <li>If the pre-commit hooks fail when making a commit, it will give you the opportunity to open the git logs to view the errors. If you are unable to fix the errors for whatever reason, you can always make a new commit from the command line with <code>--no-verify</code>:     <pre><code>git commit --no-verify -m \"Your commit message\"\n</code></pre></li> </ol>"},{"location":"code/github-project-management/","title":"GitHub and Project Management","text":""},{"location":"code/github-project-management/#guidelines","title":"Guidelines","text":"<p>These guidelines are meant to ensure that issues are created in a standardized format and contain sufficient detail so we know what needs to be done, how much effort is involved, and how to prioritize it against other issues.</p> <ol> <li>Title: This can be short, but should give enough detail to identify what work this issue is related to and differentiate it from similar task. For example, \u201cReview Documentation for [Specific Project Name]\u201d, as opposed to \u201cReview Documentation\u201d which is too vague.</li> <li> <p>Description: This should be detailed enough that anyone viewing the issue knows what work needs to be done, and what the ideal outcome or deliverable is once this work is complete (success criteria).</p> <ol> <li>If the work can be broken out into subtasks, make use of the checklist format option to list those subtasks. </li> <li>Meta-issues: If the issue is sized Large (1-2 weeks of work) or X-Large (more than 2 weeks of work), turn it into a meta-issue. The subtasks should be turned into their own issues and linked back to this issue. This should also be done if subtasks need to be assigned to someone other than the primary driver of the issue.</li> <li>Sub-task issues should be sized appropriately so that they can be completed in &lt;1 sprint of work. So Tiny (1 hour), Small (1 day), and Medium (1 week) are appropriate sizes for sub-tasks. A Large (2+ weeks) or X-Large (months) task should be broken up into smaller components and turned into a meta-issue.</li> <li>Example Meta-Issue: Note that in the screenshot below, there is text between the check-box and the issue name. This will prevent the automation that checks the box when the issue is closed from working. To enable that automation, there should be no text in the check-box besides the issue name. </li> </ol> </li> <li> <p>The following items should always be filled in when creating a new issue. If you are unsure what to enter, reach out to the Project Manager to discuss, or bring it up in daily standup or weekly sprint planning meetings to discuss with the team.</p> <ol> <li>Assignee: If you are unsure who to assign the item to, assign it to yourself to avoid losing track of the issue. Then add a comment that you think it may need to be assigned to someone else and we can discuss in the next team meeting.</li> <li>Priority: Indicates how soon the work needs to start. See definitions in the appendix at the end of this document.</li> <li>Size: Indicates how much time the work will take to complete once it has started. See definitions in the appendix at the end of this document.</li> <li>Sprint: We track work using two-week sprint cycles. Sprint names follow the a convention of [current year]-[sequential sprint number]. So the first sprint of the year 2025 is \"2025-01\". The last sprint of the year 2025 is \"2025-26\".<ol> <li>If work on an issue or task will start within the next 2 sprints, assign it to the appropriate sprint during Sprint Planning meetings. Work planned further than 2 sprints out does not need to be assigned to a sprint and can be left in the backlog, unless there is a known due date or we want to make sure something starts at a certain date.</li> </ol> </li> <li>Project: This field is important because it ensures the issue is grouped with the correct project in our Project Board. If you are unsure of the project to choose, the Project Manager can help.<ol> <li>Projects associated with DIF work start with \u201cDIF - \u201c followed by the project name.</li> <li>Tasks related to overall ODI infrastructure changes not tied to a specific project can go in the appropriate Infrastructure projects. This should be used by DSE team members only.</li> </ol> </li> <li>Milestone: Most DIF projects have Milestones which map to our project timelines. There is a search function which can help identify the correct milestone for the work being done. The Project Manager can help if you are not sure of the correct milestone.<ol> <li>Note: If a project repo does not have milestones created (for example the ODI data-infrastructure repo) then this can be left blank.</li> </ol> </li> <li>Due Date: Assign a due date when the work needs to be completed by a certain date, if applicable. If there is no set due date this can be left blank.</li> <li>Status: Assign a status on the current state of the issue. See below for status options.</li> </ol> </li> <li>Pull Requests: Add phrase \u201c(Fixes/Closes/Resolves) #xx\u201d to ensure the referenced issue gets closed when PR is merged<ol> <li>We will review \u201cOpen PRs\u201d tab during stand up.</li> </ol> </li> <li>Make sure to fully close an issue when completed by scrolling to the bottom and choosing \"Close Issue\".</li> </ol>"},{"location":"code/github-project-management/#size-definitions","title":"Size Definitions","text":"<p>Tiny</p> <ul> <li>Easy task, something that can be completed in about an hour. If something takes less than 30 minutes, no need for an issue unless it will help serve as a reminder to complete something.</li> <li>Example: Sending a follow-up email</li> </ul> <p>Small</p> <ul> <li>A task that can be completed in one day</li> <li>Example: Reviewing documentation on an API to determine how to onboard a new data source.</li> </ul> <p>Medium</p> <ul> <li>A task that can be completed in about a week</li> <li>More complex but still straightforward to complete</li> <li>Example: Creating a staging or intermediate data model for a source that is well-understood and does not require complex logic.</li> </ul> <p>Large</p> <ul> <li>A task that can be completed in 2 or more weeks</li> <li>Increased complexity, may require slower manual work or unknowns that require research and discovery</li> <li>Example: Writing an API script for a new, unknown data source.</li> </ul> <p>X-Large</p> <ul> <li>Tasks that may take a month or longer to complete</li> <li>Consider breaking up into sub-tasks if possible, or create a meta-issue when there are dependencies</li> <li>Example: Building a full set of end-to-end models for a new data source, including staging, intermediate, and mart models.</li> </ul>"},{"location":"code/github-project-management/#priority-definitions","title":"Priority Definitions","text":"<p>Urgent</p> <ul> <li>Top priority, usually reserved for unexpected emergency item.</li> <li>Putting everything else on backburner, aiming for completion ASAP</li> <li>Not assigned as part of regular sprint planning process</li> </ul> <p>High</p> <ul> <li>Goal is to complete this sprint</li> <li>Must stick to Due Date if established</li> </ul> <p>Medium</p> <ul> <li>Less important than High, aim to complete within 2-4 sprints</li> <li>OK to push to next sprint if needed</li> </ul> <p>Low</p> <ul> <li>Work on as time allows, may leave in backlog</li> </ul>"},{"location":"code/github-project-management/#status-definitions","title":"Status Definitions","text":"<ol> <li>New: Default for new issues. If discussion is needed to clarify details about the issue leave it as New until that discussion happens (so it appears in the \u201cNew Status\u201d Project Board view).</li> <li>Backlog: If work is not planned on this within the next 2-3 sprints, set the Status to Backlog (or Blocked if the work is blocked for any reason) and do not assign it to a sprint.</li> <li>TODO: If the issue is assigned to a sprint but work has not started yet, set the status as TODO.</li> <li>In Progress: Choose this status once work has started on the issue.</li> <li>Needs Review: Indicates that review is needed from other team members.</li> <li>Blocked: Choose this if work on the issue cannot move forward due to something outside of your team\u2019s control, for example if waiting on another team to respond to a question.</li> <li>Paused: Work has started on this task but has been put on hold due to competing priorities.</li> <li>Done: Indicates the issue is completed. This status is set automatically when the issue is closed, you do not need to select it.<ol> <li>Note: The correct way to close an issue is to scroll to the bottom of the issue page and choose \u201cClose Issue\u201d.</li> </ol> </li> </ol>"},{"location":"code/local-setup/","title":"Repository setup","text":"<p>These are instructions for individual contributors to set up the repository locally. For instructions on how to develop using GitHub Codespaces, see here.</p>"},{"location":"code/local-setup/#install-dependencies","title":"Install dependencies","text":"<p>Much of the software in this project is written in Python. It is usually a good idea to install Python packages into a virtual environment, which allows them to be isolated from those in other projects which might have different version constraints.</p>"},{"location":"code/local-setup/#1-install-uv","title":"1. Install <code>uv</code>","text":"<p>We use <code>uv</code> to manage our Python virtual environments. If you have not yet installed it on your system, you can follow the instructions for it here. Most of the ODI team uses Homebrew to install the package. We do not recommend installing <code>uv</code> using <code>pip</code>: as a tool for managing Python environments, it makes sense for it to live outside of a particular Python distribution.</p>"},{"location":"code/local-setup/#2-install-python-dependencies","title":"2. Install Python dependencies","text":"<p>If you prefix your commands with <code>uv run</code> (e.g. <code>uv run dbt build</code>), then <code>uv</code> will automatically make sure that the appropriate dependencies are installed before invoking the command.</p> <p>However, if you want to explicitly ensure that all of the dependencies are installed in the virtual environment, run <pre><code>uv sync\n</code></pre> in the root of the repository.</p> <p>Once the dependencies are installed, you can also \"activate\" the virtual environment (similar to how conda virtual environments are activated) by running <pre><code>source .venv/bin/activate\n</code></pre> from the repository root. With the environment activated, you no longer have to prefix commands with <code>uv run</code>.</p> <p>Which approach to take is largely a matter of personal preference:</p> <ul> <li>Using the <code>uv run</code> prefix is more reliable, as dependencies are always resolved before executing.</li> <li>Using <code>source .venv/bin/activate</code> involves less typing.</li> </ul> <p>Note</p> <p><code>uv sync</code> may not work with certain network configurations. When SSL errors are encountered, uv command line arguments can cautiously be used as a work around. <pre><code>uv sync --native-tls\nuv sync --native-tls --allow-insecure-host pypi.org --allow-insecure-host files.pythonhosted.org\n</code></pre></p>"},{"location":"code/local-setup/#3-install-go-dependencies","title":"3. Install go dependencies","text":"<p>We use Terraform to manage infrastructure. Dependencies for Terraform (mostly in the go ecosystem) can be installed via a number of different package managers.</p> <p>If you are running Mac OS, you can install these dependencies with Homebrew:</p> <pre><code>brew install terraform terraform-docs tflint go\n</code></pre> <p>If you are a conda user on any architecture, you should be able to install these dependencies with:</p> <pre><code>conda install -c conda-forge terraform go-terraform-docs tflint\n</code></pre>"},{"location":"code/local-setup/#configure-snowflake","title":"Configure Snowflake","text":"<p>In order to use Snowflake (as well as the terraform validators for the Snowflake configuration) you should set some default local environment variables in your environment. This will depend on your operating system and shell. For Linux and Mac OS systems, as well as users of Windows subsystem for Linux (WSL) it's often set in <code>~/.zshrc</code>, <code>~/.bashrc</code>, or <code>~/.bash_profile</code>.</p> <p>If you use zsh or bash, open your shell configuration file, and add the following lines:</p> <p>Default Transformer role</p> <pre><code># Legacy account identifier\nexport SNOWFLAKE_ACCOUNT=&lt;account-locator&gt;\n# The preferred account identifier is to use name of the account prefixed by its organization (e.g. myorg-account123)\n# Supporting snowflake documentation - https://docs.snowflake.com/en/user-guide/admin-account-identifier\nexport SNOWFLAKE_ACCOUNT=&lt;org_name&gt;-&lt;account_name&gt; # format is organization-account\nexport SNOWFLAKE_DATABASE=TRANSFORM_DEV\nexport SNOWFLAKE_USER=&lt;your-username&gt; # this should be your OKTA email\nexport SNOWFLAKE_PASSWORD=&lt;your-password&gt; # this should be your OKTA password\nexport SNOWFLAKE_ROLE=TRANSFORMER_DEV\nexport SNOWFLAKE_WAREHOUSE=TRANSFORMING_XS_DEV\nexport SNOWFLAKE_AUTHENTICATOR=ExternalBrowser\n</code></pre> <p>Open a new terminal and verify that the environment variables are set.</p> <p>Switch to Loader role</p> <pre><code># Legacy account identifier\nexport SNOWFLAKE_ACCOUNT=&lt;account-locator&gt;\n# The preferred account identifier is to use name of the account prefixed by its organization (e.g. myorg-account123)\n# Supporting snowflake documentation - https://docs.snowflake.com/en/user-guide/admin-account-identifier\nexport SNOWFLAKE_ACCOUNT=&lt;org_name&gt;-&lt;account_name&gt; # format is organization-account\nexport SNOWFLAKE_DATABASE=RAW_DEV\nexport SNOWFLAKE_USER=&lt;your-username&gt; # this should be your OKTA email\nexport SNOWFLAKE_PASSWORD=&lt;your-password&gt; # this should be your OKTA password\nexport SNOWFLAKE_ROLE=LOADER_DEV\nexport SNOWFLAKE_WAREHOUSE=LOADING_XS_DEV\nexport SNOWFLAKE_AUTHENTICATOR=ExternalBrowser\n</code></pre> <p>This will enable you develop scripts for loading raw data into the development environment. Again, open a new terminal and verify that the environment variables are set.</p>"},{"location":"code/local-setup/#configure-aws-optional","title":"Configure AWS (optional)","text":"<p>In order to create and manage AWS resources programmatically, you need to create access keys and configure your local setup to use them:</p> <ol> <li>Install the AWS command-line interface.</li> <li>Go to the AWS IAM console and create an access key for yourself.</li> <li>In a terminal, enter <code>aws configure</code>, and add the access key ID and secret access key when prompted. We use <code>us-west-2</code> as our default region.</li> </ol>"},{"location":"code/local-setup/#configure-dbt","title":"Configure dbt","text":"<p>dbt core is automatically installed when you set up the environment with <code>uv</code>. The connection information for our data warehouses will, in general, live outside of this repository. This is because connection information is both user-specific and usually sensitive, so it should not be checked into version control.</p> <p>In order to run this project locally, you will need to provide this information in a YAML file. Run the following command to create the necessary folder and file.</p> <pre><code>mkdir ~/.dbt &amp;&amp; touch ~/.dbt/profiles.yml\n</code></pre> <p>Note</p> <p>This will only work on posix-y systems. Windows users will have a different command (except in Powershell).</p> <p>Instructions for writing a <code>profiles.yml</code> are documented here, there are specific instructions for Snowflake here, and you can find examples for ODI and external users below as well.</p> <p>You can verify that your <code>profiles.yml</code> is configured properly by running the following command in the project root directory (<code>transform</code>).</p> <pre><code>dbt debug\n</code></pre>"},{"location":"code/local-setup/#snowflake-project","title":"Snowflake project","text":"<p>A minimal version of a <code>profiles.yml</code> for dbt development is:</p> <p>ODI users <pre><code>dse_snowflake:\n  target: dev\n  outputs:\n    dev:\n      type: snowflake\n      account: &lt;account-locator&gt;\n      user: &lt;your-innovation-email&gt;\n      authenticator: externalbrowser\n      role: TRANSFORMER_DEV\n      database: TRANSFORM_DEV\n      warehouse: TRANSFORMING_XS_DEV\n      schema: DBT_&lt;your-name&gt;   # Test schema for development\n      threads: 4\n</code></pre></p> <p>External users <pre><code>dse_snowflake:\n  target: dev\n  outputs:\n    dev:\n      type: snowflake\n      account: &lt;account-locator&gt;\n      user: &lt;your-username&gt;\n      password: &lt;your-password&gt;\n      authenticator: username_password_mfa\n      role: TRANSFORMER_DEV\n      database: TRANSFORM_DEV\n      warehouse: TRANSFORMING_XS_DEV\n      schema: DBT_&lt;your-name&gt;   # Test schema for development\n      threads: 4\n</code></pre></p> <p>Note</p> <p>The target name (<code>dev</code>) in the above example can be anything. However, we treat targets named <code>prd</code> differently in generating custom dbt schema names (see here). We recommend naming your local development target <code>dev</code>, and only include a <code>prd</code> target in your profiles under rare circumstances.</p>"},{"location":"code/local-setup/#combined-profilesyml","title":"Combined <code>profiles.yml</code>","text":"<p>You can include profiles for several databases in the same <code>profiles.yml</code>, (as well as targets for production), allowing you to develop in several projects using the same computer.</p>"},{"location":"code/local-setup/#handling-ssl-errors","title":"Handling SSL errors","text":"<p>With certain network configurations, running <code>dbt deps</code> may result in SSL certificate errors. In such cases, installing the <code>pip-system-certs</code> package can help Python-based dbt installations work around these errors.</p>"},{"location":"code/local-setup/#example-vs-code-setup","title":"Example VS Code setup","text":"<p>This project can be developed entirely using dbt Cloud. That said, many people prefer to use more featureful editors, and the code quality checks that are set up here are easier to run locally. By equipping a text editor like VS Code with an appropriate set of extensions and configurations we can largely replicate the dbt Cloud experience locally. Here is one possible configuration for VS Code:</p> <ol> <li>Install some useful extensions (this list is advisory, and non-exhaustive):<ul> <li>dbt Power User (query previews, compilation, and auto-completion)</li> <li>Python (Microsoft's bundle of Python linters and formatters)</li> <li>sqlfluff (SQL linter)</li> </ul> </li> <li>Configure the VS Code Python extension to use your virtual environment by choosing <code>Python: Select Interpreter</code> from the command palette and selecting your virtual environment (<code>infra</code>) from the options.</li> <li>Associate <code>.sql</code> files with the <code>jinja-sql</code> language by going to <code>Code</code> -&gt; <code>Preferences</code> -&gt; <code>Settings</code> -&gt; <code>Files: Associations</code>, per these instructions.</li> <li>Test that the <code>vscode-dbt-power-user</code> extension is working by opening one of the project model <code>.sql</code> files and pressing the \"\u25b6\" icon in the upper right corner. You should have query results pane open that shows a preview of the data.</li> </ol>"},{"location":"code/local-setup/#installing-pre-commit-hooks","title":"Installing <code>pre-commit</code> hooks","text":"<p>This project uses pre-commit to lint, format, and generally enforce code quality. These checks are run on every commit, as well as in CI.</p> <p>To set up your pre-commit environment locally run the following in the <code>data-infrastructure</code> repo root folder:</p> <pre><code>pre-commit install\n</code></pre> <p>The next time you make a commit, the pre-commit hooks will run on the contents of your commit (the first time may be a bit slow as there is some additional setup).</p> <p>You can verify that the pre-commit hooks are working properly by running</p> <p><pre><code>pre-commit run --all-files\n</code></pre> to test every file in the repository against the checks.</p> <p>Some of the checks lint our dbt models and Terraform configurations, so having the terraform dependencies installed and the dbt project configured is a requirement to run them, even if you don't intend to use those packages.</p>"},{"location":"code/terraform-local-setup/","title":"Terraform Setup","text":""},{"location":"code/terraform-local-setup/#installation","title":"Installation","text":"<p>This project requires Terraform to run. You might use a different package manager to install it depending on your system.</p> <p>For Macs, you can use <code>brew</code>:</p> <pre><code>brew install terraform\n</code></pre> <p>Anaconda users on any architecture should be able to use <code>conda</code> or <code>mamba</code>:</p> <pre><code>conda install -c conda-forge terraform\n</code></pre> <p>We also use <code>tflint</code> for linting, and <code>terraform-docs</code> to help with documentation of resources. These can be installed in the same manner, e.g.:</p> <pre><code>conda install -c conda-forge tflint go-terraform-docs\n</code></pre> <p>There are a number of pre-commit checks that run on committing as well as in CI. To install the checks, run the following from the repository root:</p> <pre><code>pre-commit install\n</code></pre> <p>You can manually run the pre-commit checks using:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"code/terraform-local-setup/#bootstrapping-remote-state","title":"Bootstrapping remote state","text":"<p>When deploying a new version of your infrastrucutre, Terraform diffs the current state against what you have specified in your infrastructure-as-code. The current state is tracked in a JSON document, which can be stored in any of a number of locations (including local files).</p> <p>This project stores remote state using the S3 backend.</p> <p>Different applications or environments can be isolated from each other by using different S3 buckets for holding their state. We reuse a terraform configuration (<code>terraform/s3-remote-state</code>) for setting up the S3 backend</p> <p>Note</p> <p>The S3 remote state configuration is not a proper module because it contains a provider block. Different deployments of the configuration are controlled by giving it different <code>tfvars</code> files, and capturing the outputs for use in a <code>tfbackend</code> file.</p> <p>Here is an example set of commands for bootstrapping a new S3 backend for a deployment. Suppose the deployment is a QA environment of our Snowflake project:</p> <pre><code>cd terraform/snowflake/environments/qa  # Go to the new environment directory\nmkdir remote-state  # Create a remote-state directory\ncd remote-state\nln -s ../../../s3-remote-state/main.tf main.tf  # symlink the s3 configuration\nterraform init  # initialize the remote state backend\nterraform apply -var=\"owner=dse\" -var=\"environment=qa\" -var=\"project=snowflake\"  # Create the infrastructure\nterraform output &gt; ../dse-snowflake-qa.tfbackend  # Pipe the outputs to a .tfbackend\n\ncd ..\nterraform init -backend-config=./dse-snowflake-qa.tfbackend  # Configure the deployment with the new backend.\n</code></pre>"},{"location":"code/terraform-local-setup/#deploying-infrastructure","title":"Deploying Infrastructure","text":"<p>When you are ready to deploy a new version of the infrastructure, run</p> <pre><code>terraform apply\n</code></pre> <p>This will output the changes to the infrastructure that will be made, and prompt you for confirmation.</p>"},{"location":"code/terraform-local-setup/#updating-terraform-dependencies","title":"Updating terraform dependencies","text":"<p>Terraform deployments include a lockfile with hashes of installed packages. Because we have mixed development environments (i.e., Macs locally, Linux in CI), it is helpful to include both Mac and Linux builds of terraform packages in the lockfile. This needs to be done every time package versions are updated:</p> <pre><code>terraform init -upgrade  # Upgrade versions\nterraform providers lock -platform=linux_amd64 -platform=darwin_amd64  # include Mac and Linux binaries\n</code></pre>"},{"location":"code/terraform-local-setup/#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws 4.56.0 random 3.4.3"},{"location":"code/terraform-local-setup/#providers","title":"Providers","text":"Name Version aws 4.56.0 random 3.4.3"},{"location":"code/terraform-local-setup/#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"code/terraform-local-setup/#resources","title":"Resources","text":"Name Type aws_batch_compute_environment.default resource aws_batch_job_definition.default resource aws_batch_job_queue.default resource aws_ecr_repository.default resource aws_eip.this resource aws_iam_group.aae resource aws_iam_group_membership.aae resource aws_iam_group_policy_attachment.aae_dsa_project resource aws_iam_group_policy_attachment.aae_list_all_my_buckets resource aws_iam_group_policy_attachment.aae_self_manage_creentials resource aws_iam_policy.access_snowflake_loader resource aws_iam_policy.batch_submit_policy resource aws_iam_policy.default_ecr_policy resource aws_iam_policy.dof_demographics_read_write_access resource aws_iam_policy.mwaa resource aws_iam_policy.s3_dsa_project_policy resource aws_iam_policy.s3_list_all_my_buckets resource aws_iam_policy.s3_scratch_policy resource aws_iam_policy.self_manage_credentials resource aws_iam_role.aws_batch_service_role resource aws_iam_role.batch_job_role resource aws_iam_role.ecs_task_execution_role resource aws_iam_role.mwaa resource aws_iam_role_policy_attachment.aws_batch_service_role resource aws_iam_role_policy_attachment.dof_demographics_read_write_access resource aws_iam_role_policy_attachment.ecs_task_execution_access_snowflake_loader resource aws_iam_role_policy_attachment.ecs_task_execution_role_policy resource aws_iam_role_policy_attachment.mwaa_batch_submit_role resource aws_iam_role_policy_attachment.mwaa_execution_role resource aws_iam_role_policy_attachment.s3_scratch_policy_role_attachment resource aws_iam_user.arman resource aws_iam_user.cd_bot resource aws_iam_user.esa resource aws_iam_user.kim resource aws_iam_user.monica resource aws_iam_user.rocio resource aws_iam_user_policy_attachment.batch_cd_bot_policy_attachment resource aws_iam_user_policy_attachment.ecr_cd_bot_policy_attachment resource aws_internet_gateway.this resource aws_mwaa_environment.this resource aws_nat_gateway.this resource aws_route_table.private resource aws_route_table.public resource aws_route_table_association.private resource aws_route_table_association.public resource aws_s3_bucket.dof_demographics_public resource aws_s3_bucket.dsa_project resource aws_s3_bucket.mwaa resource aws_s3_bucket.scratch resource aws_s3_bucket_policy.dof_demographics_public_read_access resource aws_s3_bucket_public_access_block.dof_demographics_public resource aws_s3_bucket_public_access_block.mwaa resource aws_s3_bucket_versioning.dof_demographics_public resource aws_s3_bucket_versioning.dsa_project resource aws_s3_bucket_versioning.mwaa resource aws_security_group.batch resource aws_security_group.mwaa resource aws_subnet.private resource aws_subnet.public resource aws_vpc.this resource random_id.private_subnet resource random_id.public_subnet resource aws_availability_zones.available data source aws_caller_identity.current data source aws_iam_policy_document.access_snowflake_loader data source aws_iam_policy_document.assume data source aws_iam_policy_document.assume_role_policy data source aws_iam_policy_document.aws_batch_service_policy data source aws_iam_policy_document.batch_submit_policy_document data source aws_iam_policy_document.default_ecr_policy_document data source aws_iam_policy_document.dof_demographics_public_read_access data source aws_iam_policy_document.dof_demographics_read_write_access data source aws_iam_policy_document.mwaa data source aws_iam_policy_document.s3_dsa_project_policy_document data source aws_iam_policy_document.s3_list_all_my_buckets data source aws_iam_policy_document.s3_scratch_policy_document data source aws_iam_policy_document.self_manage_credentials data source aws_secretsmanager_secret.snowflake_loader_secret data source"},{"location":"code/terraform-local-setup/#inputs","title":"Inputs","text":"Name Description Type Default Required environment Deployment environment of the resource <code>string</code> <code>\"dev\"</code> no owner Owner of the resource <code>string</code> <code>\"dse\"</code> no project Name of the project the resource is serving <code>string</code> <code>\"infra\"</code> no region Region for AWS resources <code>string</code> <code>\"us-west-2\"</code> no snowflake_loader_secret ARN for SecretsManager login info to Snowflake with loader role <code>object({ test = string, latest = string })</code> <code>null</code> no"},{"location":"code/terraform-local-setup/#outputs","title":"Outputs","text":"Name Description state Resources from terraform-state"},{"location":"code/working-with-snowflake-objects/","title":"Working with Snowflake Code Artifacts","text":"<p>Snowflake has a few first-class code objects which are stored in databases:</p> <ul> <li>Notebooks: notebook files (<code>.ipynb</code>) allow for mixing SQL statements,     Python code, narrative prose, and visualizations. They can be useful for exploratory     data analysis, and as a means for sharing data driven documents.</li> <li>Streamlit dashboards: Streamlit dashboards allow for interactive data visualizations     which can be shared with the organization.</li> </ul> <p>A downside of these objects is that it is more difficult to track, version, and maintain high code quality for them. This document includes some recommendations and best practices for working with notebooks and Streamlit dashboards within Snowflake.</p> <p>Note</p> <p>The DOE team's thinking about whether/when to use these objects is still a moving target. These recommendations are weakly-held, and subject to change.</p>"},{"location":"code/working-with-snowflake-objects/#store-notebooks-and-dashboards-in-the-analytics-layer","title":"Store notebooks and dashboards in the <code>ANALYTICS</code> layer","text":"<p>Since notebooks and dashboards are most frequently used as reporting products, they are most appropriate for the <code>ANALYTICS</code>/marts layer. This means:</p> <ol> <li>Create them in the <code>ANALYTICS_{env}</code> database.</li> <li>Create them using the <code>REPORTER_{env}</code> role.</li> <li>Use the <code>REPORTING_XS_{env}</code> warehouse for queries.</li> </ol> <p>Using the <code>REPORTER</code> role means that the dashboard will not be able to access data from the <code>RAW</code> or <code>TRANSFORM</code> layers, nor will it be able to edit data in the <code>ANALYTICS</code> layer. This is a useful security feature, and consistent with our RBAC design.</p> <p>One consequence of this recommendation is that every table/view used by the notebook or dashboard should also be in the <code>ANALYTICS</code> layer. In development it may be useful to use the <code>TRANSFORMER</code> role, but \"production\" dashboards should endeavor to use <code>REPORTER_PRD</code>. Getting the appropriate data in <code>ANALYTICS</code> for consumption may take some extra work, we recommend using the guidelines here to assess when it is appropriate.</p> <p>Warning</p> <p>Streamlit dashboards execute using the role of their owner. Malicious users with edit permissions on the dashboard may be able to use them for privilege escalation if the dashboard is owned by a powerful role. Even for users with good intentions, it may be possible to accidentally cause problems. Using the <code>REPORTER_{env}</code> role is for Streamlit dashboards is an application of the principle of least privilege. For more discussion of Streamlit dashboard security, see these docs.</p>"},{"location":"code/working-with-snowflake-objects/#use-extra-small-warehouses","title":"Use extra-small warehouses","text":"<p>Notebooks and Streamlit dashboards involve two separate Snowflake warehouses, the one powering their Python kernel, and the one powering their SQL queries. The Python kernel is active as long as the notebook/dashboard is active, while the query warehouse may auto-suspend when no queries are being executed. As such, it's not unusual for the warehouse cost of running and developing the notebook/dashboard to be more than  that of the warehouse cost of its queries.</p> <p>To save on compute and avoid surprise cost overruns, always use the default (extra-small) <code>SYSTEM$STREAMLIT_NOTEBOOK_WH</code> for the kernel, and select an extra small warehouse for queries. If more expensive queries are needed, consider pre-building tables in the <code>ANALYTICS</code> database and making sure appropriate filters are used to cut down on data scans.</p>"},{"location":"code/working-with-snowflake-objects/#use-version-control","title":"Use version control","text":"<p>Use of version control with notebooks and Streamlit dashboards is still important. We recommend ensuring that any object be in version control and fully reproducible if any of the following are true:</p> <ol> <li>It is shared outside of the immediate DOE team.</li> <li>It is used for any recommendation, publication, or data product.</li> <li>It is used operationally (i.e., someone looks in on it regularly).</li> </ol>"},{"location":"code/working-with-snowflake-objects/#connecting-a-repository","title":"Connecting a repository","text":"<p>Note: this only needs to be done once per Snowflake environment and GitHub repository. Specific roles and permission grants are provisional, and it would be nice to clean it up a bit</p> <ol> <li>Create a scoped fine-grained personal access token for repository access.     This fine-grained token only needs Read-only access to the relevant repository.     It's worthwhile to keep the token scope minimal, as everyone with access to the     git repository in Snowflake will be using this token.</li> <li>Create development and production versions of the repository in Snowflake. The following SQL     script can be used to set up the repos. Depending on the situation, some     parts of it may not be necessary, or the repository may need to be created in     a different set of databases/schemas, so read through with some care.     <pre><code>use role accountadmin;\n\n/* First, we create secrets for the GitHub authentication\n   as well as the API integration so we can talk to GitHub\n   We can use a fine-grained token that only has read access\n   to a single repository for safety. Still not ideal that it's a user\n   token rather than an OAuth app or similar, but that seems to\n   be the only way to do it.\n*/\ncreate or replace secret analytics_prd.public.gh_token\ntype = password\nusername = '&lt;GitHub username&gt;'\npassword = '&lt;token here&gt;';\n\ncreate or replace secret analytics_dev.public.gh_token\ntype = password\nusername = '&lt;GitHub username&gt;'\npassword = '&lt;token here&gt;';\n\ncreate or replace api integration cagov_github\napi_provider = git_https_api\napi_allowed_prefixes = ('https://github.com/cagov')\nallowed_authentication_secrets = ('analytics_dev.public.gh_token', 'analytics_prd.public.gh_token')\nenabled = true;\n\n-- Grant usage on the secret and API integration to the transformer roles\ngrant usage on integration cagov_github to role transformer_dev;\ngrant usage on secret analytics_dev.public.gh_token to role transformer_dev;\ngrant usage on integration cagov_github to role transformer_prd;\ngrant usage on secret analytics_prd.public.gh_token to role transformer_prd;\n\n-- Grant the ability to create GitHub repos and Streamlit apps on PUBLIC to the transformer roles.\n-- These are things we can move to our default terraform configuration.\nuse role sysadmin;\ngrant create git repository on schema analytics_dev.public to role transformer_dev;\ngrant create git repository on schema analytics_prd.public to role transformer_prd;\ngrant create streamlit on schema analytics_dev.public to role transformer_dev;\ngrant create streamlit on schema analytics_prd.public to role transformer_prd;\n\n\n-- Create the Git reepositories in dev and prod\nuse role transformer_dev;\ncreate or replace git repository analytics_dev.public.\"&lt;repo-name&gt;\"\norigin = 'https://github.com/cagov/&lt;repo-name&gt;'\napi_integration = 'CAGOV_GITHUB'\ngit_credentials = 'analytics_dev.public.gh_token';\n\nuse role transformer_prd;\ncreate or replace git repository analytics_prd.public.\"&lt;repo-name&gt;\"\norigin = 'https://github.com/cagov/&lt;repo-name&gt;'\napi_integration = 'CAGOV_GITHUB'\ngit_credentials = 'analytics_prd.public.gh_token';\n</code></pre></li> </ol> <p>This repository can now be used with notebooks and Streamlit dashboards.</p>"},{"location":"code/working-with-snowflake-objects/#getting-a-snowflake-notebook-or-dashboard-into-a-git-repository","title":"Getting a Snowflake notebook or dashboard into a git repository","text":"<p>There are two different scenarios we want to address here:</p> <ol> <li>Creating a new notebook or dashboard in Snowflake based on an existing file in a Git repository</li> <li>Adding an existing notebook or dashboard in Snowflake to a Git repository</li> </ol> <p>In both cases, you should have a <code>REPORTER_{env}</code> role selected in the Snowflake role switcher in the bottom left.</p> <p>Warning</p> <p>Somewhat annoyingly, it's difficult-to-impossible to change the name of notebooks and dashboards after they are created in Snowflake, as well as their paths in a Git repository. When choosing file names, we recommend the following:</p> <ul> <li>Use simple, descriptive file names describing what the notebook/dashboard does.</li> <li>Do not use Snowflake unique identifiers for file names: these will not be stable between branches.</li> <li>Avoid any references to things like \"prod\" or \"final\" or \"use this one\" in the name:     these will quickly become incorrect with a branching git-based workflow.</li> </ul> <p>As a prerequisite to this, you should create a GitHub personal access token. allowing you to make commits within the Snowflake UI under your name. We recommend creating a fine-grained token, with \"Read and Write\" access to \"Contents\" for the relevant repositories. Creating this token will likely need approval from a GitHub organization admin.</p>"},{"location":"code/working-with-snowflake-objects/#creating-a-new-notebookdashboard-in-snowflake-based-on-an-existing-file-in-a-git-repository","title":"Creating a new notebook/dashboard in Snowflake based on an existing file in a Git repository","text":"<ol> <li>Create a new branch for the object in your git repository     (the Snowflake UI doesn't have a way to create branches):     <pre><code>git switch -c &lt;new-branch-name&gt;\n</code></pre></li> <li>In the upper left of the notebook/Streamlit page click the \"down\" caret     and choose \"Create from repository\"</li> <li>Give the notebook/Streamlit a descriptive name. Warning: dashboards and notebooks cannot     have their name changed after the fact. Be sure to choose wisely, otherwise     you may need to re-create it in a new branch</li> <li>In \"File location in the repository\", find the Git repository in Snowflake,     select the branch you created above,     and navigate to the path you want for the notebook/Streamlit in the repository.     For instance, we might have a <code>notebooks</code> or <code>streamlit</code> subdirectory.</li> <li>Choose the database location for the object (e.g., <code>ANALYTICS_DEV.PUBLIC</code>)</li> <li>Choose the warehouse for the object (e.g., <code>REPORTING_XS_DEV</code>).</li> </ol>"},{"location":"code/working-with-snowflake-objects/#adding-an-existing-notebookdashboard-in-snowflake-to-a-git-repository","title":"Adding an existing notebook/dashboard in Snowflake to a Git repository","text":"<p>Warning</p> <p>We currently do not recommend adding an existing Streamlit dashboard to a Git repository. This is because the dashboard name is a unique identifier that becomes the folder name in the repository. At the time of this writing, it is impossible to change this, which means that the folder name is both non-descriptive and doesn't work with a standard git-based branching workflow.</p> <p>For whatever reason, Snowflake notebooks to not have this defect: as long as you give the notebook a descriptive name upon creating it, the folder name in the git repository will match it.</p> <ol> <li>In the left side panel for the notebook/Streamlit, click the \"Connect Git Repository\" button</li> <li>In \"File location in the repository\", find the Git repository in Snowflake,     select the branch you created above,     and navigate to the path you want for the notebook/Streamlit in the repository.     For instance, we might have a <code>notebooks</code> or <code>streamlit</code> subdirectory.</li> <li>Add a commit message and push the commit adding the object to GitHub.     If this is your first commit in Snowflake you will need to input your     personal access token.</li> </ol>"},{"location":"code/working-with-snowflake-objects/#devprod-promotion","title":"Dev/Prod promotion","text":"<p>A large part of the benefit of getting notebooks and dashboards into version control is that you can use a standard git-based branching workflow for proposing changes. The following is a recipe for such a workflow. We describe the workflow for a dashboard for brevity, but it should be similar for notebooks:</p>"},{"location":"code/working-with-snowflake-objects/#propose-a-new-notebookdashboard","title":"Propose a new notebook/dashboard","text":"<ol> <li>Create a new branch for the dashboard:    <code>git switch -c feature-new-dashboard</code></li> <li>Enable the <code>REPORTER_DEV</code> role in Snowflake.</li> <li>The next step is a little different for notebooks and dashboards.<ol> <li>If creating a dashboard:     Create a folder and file for the dashboard in the repository.     This is important because Snowflake makes it very difficult/impossible to change     the name after the fact. Creating one with the right name at the outset gives you     the most control, and avoids files with inscrutible timestamps or unique IDs in them! <pre><code>mkdir -p streamlit/my_new_dashboard\ntouch streamlit/my_new_dashboard/my_new_dashboard.py\ngit commit streamlit/my_new_dashboard/my_new_dashboard.py -m \"Added new file\"\ngit push &lt;remote-name&gt; feature-new-dashboard\n</code></pre>     Connect to the blank dashboard using the \"Creating a new...\" workflow above.</li> <li>If creating a notebook:     Create the new notebook in Snowflake, and add it to the git repository using the     \"Adding an existing...\" workflow above.     Be sure to choose a good name for the notebook, as it will become the folder     name and cannot be changed after the fact.</li> </ol> </li> <li>Develop the dashboard, making regular descriptive commits.</li> <li>When ready, create a pull request in GitHub, proposing the new dashboard     be merged to <code>main</code>!</li> </ol>"},{"location":"code/working-with-snowflake-objects/#merge-the-new-notebookdashboard-to-main","title":"Merge the new notebook/dashboard to <code>main</code>","text":"<ol> <li>Review the dashboard code with coworkers,     making any changes that come out of code review.</li> <li>When satisfied, merge the pull request to <code>main</code>.     Snowflake seemingly cannot change the branch of a dashboard after it is created.     Therefore you will need to recreate a production version based on the one in <code>main</code>.</li> <li>Enable the <code>REPORTER_PRD</code> role in Snowflake.</li> <li>Create a new dashboard in <code>ANALYTICS_PRD</code>  based on the version in <code>main</code>,     following the \"Creating a new...\" workflow.     This version is now your production dashboard!</li> <li>Delete the old development dashboard in Snowflake to prevent clutter.</li> </ol>"},{"location":"code/working-with-snowflake-objects/#propose-changes-to-the-dashboard","title":"Propose changes to the dashboard","text":"<p>Now that the dashboard is in <code>main</code>, you can propose new changes using a standard git-based workflow.</p> <ol> <li>Check out a new branch from <code>main</code>:     <pre><code>git switch -c new-feature\n</code></pre></li> <li>Create dashboard using the \"Creating a new...\" workflow above.</li> <li>Develop the dashboard, making regular descriptive commits.</li> <li>Propose your changes in a pull request to <code>main</code>.</li> <li>Once it is merged to main, you don't need to recreate the dashboard anymore,     as the production version is already pointed at <code>main</code>.     Instead, navigate to the git repository in Snowflake and click \"Fetch\",     which should bring in the new changes. The next time the dashboard     launches, it should be based on the latest version.</li> <li>Delete the development dashboard in Snowflake to prevent clutter.</li> </ol>"},{"location":"code/working-with-snowflake-objects/#where-to-manipulate-data","title":"Where to manipulate data","text":"<p>When developing notebooks or dashboards, the analyst often has to choose how much data manipulation should be done in Python, and how much should be done using SQL and dbt. In general, we prefer to transform data and build data models using dbt, rather than doing it within notebooks or dashboards. This is for a few reasons:</p> <ol> <li>dbt has a stronger version-control story than Snowflake notebooks or dashboards.</li> <li>It's easier to test and exercise code for dbt models.</li> <li>It's easier to reuse the data models developed in dbt for multiple applications.</li> <li>In general, executing a Snowflake SQL query is higher-performance than bringing data into     a Python session and manipulating it there.</li> </ol> <p>That said, if you are doing highly dynamic things in a dashboard, such as on-the-fly filtering or aggregation of data based on user interactions, then it may make sense to do some of the transformation in the Python session. There are no hard-and-fast rules here, and specific applications will have different tradeoffs! But when in doubt, we recommend you prefer doing transformations in the dbt layer. For more information about our default dbt project architecture, see here.</p>"},{"location":"code/writing-documentation/","title":"Writing Documentation","text":"<p>Documentation for this project is built using mkdocs with the material theme and hosted using GitHub Pages. The documentation source files are in the <code>docs/</code> directory and are authored using markdown.</p>"},{"location":"code/writing-documentation/#local-development","title":"Local Development","text":"<p>To write documentation for this project, make sure that the build tools are installed. In a Python environment and in the data-infrastructure repo, you should be able to start a local server for the docs by running:</p> <pre><code>mkdocs serve\n</code></pre> <p>Then open a web browser to http://localhost:8000 to view the built docs. Any edits you make to the markdown sources should be automatically picked up, and the page should automatically rebuild and refresh.</p>"},{"location":"code/writing-documentation/#deployment","title":"Deployment","text":"<p>Deployment of the docs for this repository is done automatically upon merging to <code>main</code> using the <code>docs</code> GitHub Action.</p> <p>Built documentation is pushed to the <code>gh-pages</code> branch of the repository, and can be viewed by navigating to https://cagov.github.io/data-infrastructure.</p>"},{"location":"data/footprints/","title":"Building Footprints Dataset","text":"<p>The Data Services and Engineering team maintains a derived dataset from the Microsoft US Building Footprints and Global ML Building Footprints datasets. The two datasets are broadly similar, but the latter has global coverage and is more frequently updated.</p> <p>We take the original datasets, and join them with US Census TIGER data to make them more useful for demographic and social science research. The additional census-derived fields include:</p> <ul> <li>State</li> <li>County</li> <li>Tract</li> <li>Block Group</li> <li>Block</li> <li>Place</li> </ul> <p>If a footprint intersects more than one of the above, we assign it to the one with the greater intersection, so each footprint should only appear once in the dataset.</p> <p>Note</p> <p>Despite the names, these derived datasets are scoped to California only.</p>"},{"location":"data/footprints/#usage","title":"Usage","text":"<p>The data are stored as files in AWS S3. We distribute them in both GeoParquet and zipped Shapefile formats.</p> <p>GeoParquet is usually a superior format for doing data analytics as it is:</p> <ol> <li>An open format, based on the industry-standard Parquet format.</li> <li>Efficiently compressed</li> <li>Cloud-native</li> <li>Uses a columnar data layout optimized for analytical workloads.</li> </ol> <p>However, GeoParquet is also somewhat newer, and not supported by all tooling yet, so the zipped Shapefiles may be better suited for some workflows (especially Esri ones).</p>"},{"location":"data/footprints/#usage-with-geopandas","title":"Usage with GeoPandas","text":"<p>GeoPandas is an extension to the Python Pandas library enabling analysis of geopsatial vector data.</p> <p>Examples for reading the files using GeoPanas:</p> <pre><code>import os\nimport geopandas\n\n# Ensure S3 requests are anonymous, there is no need for AWS credentials here.\nos.environ[\"AWS_NO_SIGN_REQUEST\"] = \"YES\"\n\n# Read GeoParquet using S3 URL\ngdf = geopandas.read_parquet(\n    \"s3://dof-demographics-dev-us-west-2-public/\"\n    \"global_ml_building_footprints/parquet/county_fips_003.parquet\"\n)\n\n# Read Shapefile using HTTPS URL\ngdf = geopandas.read_file(\n    \"https://dof-demographics-dev-us-west-2-public.s3.us-west-2.amazonaws.com/\"\n    \"global_ml_building_footprints/shp/county_fips_003.zip\"\n)\n</code></pre>"},{"location":"data/footprints/#usage-with-arcgis-pro-toolbox","title":"Usage with ArcGIS Pro toolbox:","text":"<p>Fennis Reed at the California Department of Finance Demographics Research Unit has created an ArcGIS Pro toolbox for downloading individual footprint files, which can be downloaded here. Some usage notes for the toolbox are here</p>"},{"location":"data/footprints/#removing-errors-of-inclusion","title":"Removing errors of inclusion:","text":"<p>Fennis Reed has also constructed a dataset of known \"errors of inclusion\", i.e. shapes that were incorrectly identified as footprints by the machine learning algorithm. These errors of inclusion are updated in this gist, and examples for how to use them to filter them from other datasets can be found in this gist.</p> <p>The above ArcGIS Pro toolbox also includes an option for filtering errors of inclusion using the same process.</p>"},{"location":"data/footprints/#links","title":"Links","text":"<p>The following tables contains public links to the datasets partitioned by county. The HTTPS URLs can be used to directly download files using a web browser, while the S3 URLs are more appropriate for scripts like the examples above.</p>"},{"location":"data/footprints/#global-ml-building-footprints","title":"Global ML Building Footprints","text":"County GeoParquet (HTTPS) Shapefile (HTTPS) GeoParquet (S3) Shapefile (S3) Alameda URL URL URL URL Alpine URL URL URL URL Amador URL URL URL URL Butte URL URL URL URL Calaveras URL URL URL URL Colusa URL URL URL URL Contra Costa URL URL URL URL Del Norte URL URL URL URL El Dorado URL URL URL URL Fresno URL URL URL URL Glenn URL URL URL URL Humboldt URL URL URL URL Imperial URL URL URL URL Inyo URL URL URL URL Kern URL URL URL URL Kings URL URL URL URL Lake URL URL URL URL Lassen URL URL URL URL Los Angeles URL URL URL URL Madera URL URL URL URL Marin URL URL URL URL Mariposa URL URL URL URL Mendocino URL URL URL URL Merced URL URL URL URL Modoc URL URL URL URL Mono URL URL URL URL Monterey URL URL URL URL Napa URL URL URL URL Nevada URL URL URL URL Orange URL URL URL URL Placer URL URL URL URL Plumas URL URL URL URL Riverside URL URL URL URL Sacramento URL URL URL URL San Benito URL URL URL URL San Bernardino URL URL URL URL San Diego URL URL URL URL San Francisco URL URL URL URL San Joaquin URL URL URL URL San Luis Obispo URL URL URL URL San Mateo URL URL URL URL Santa Barbara URL URL URL URL Santa Clara URL URL URL URL Santa Cruz URL URL URL URL Shasta URL URL URL URL Sierra URL URL URL URL Siskiyou URL URL URL URL Solano URL URL URL URL Sonoma URL URL URL URL Stanislaus URL URL URL URL Sutter URL URL URL URL Tehama URL URL URL URL Trinity URL URL URL URL Tulare URL URL URL URL Tuolumne URL URL URL URL Ventura URL URL URL URL Yolo URL URL URL URL Yuba URL URL URL URL"},{"location":"data/footprints/#us-building-footprints","title":"US Building Footprints","text":"County GeoParquet (HTTPS) Shapefile (HTTPS) GeoParquet (S3) Shapefile (S3) Alameda URL URL URL URL Alpine URL URL URL URL Amador URL URL URL URL Butte URL URL URL URL Calaveras URL URL URL URL Colusa URL URL URL URL Contra Costa URL URL URL URL Del Norte URL URL URL URL El Dorado URL URL URL URL Fresno URL URL URL URL Glenn URL URL URL URL Humboldt URL URL URL URL Imperial URL URL URL URL Inyo URL URL URL URL Kern URL URL URL URL Kings URL URL URL URL Lake URL URL URL URL Lassen URL URL URL URL Los Angeles URL URL URL URL Madera URL URL URL URL Marin URL URL URL URL Mariposa URL URL URL URL Mendocino URL URL URL URL Merced URL URL URL URL Modoc URL URL URL URL Mono URL URL URL URL Monterey URL URL URL URL Napa URL URL URL URL Nevada URL URL URL URL Orange URL URL URL URL Placer URL URL URL URL Plumas URL URL URL URL Riverside URL URL URL URL Sacramento URL URL URL URL San Benito URL URL URL URL San Bernardino URL URL URL URL San Diego URL URL URL URL San Francisco URL URL URL URL San Joaquin URL URL URL URL San Luis Obispo URL URL URL URL San Mateo URL URL URL URL Santa Barbara URL URL URL URL Santa Clara URL URL URL URL Santa Cruz URL URL URL URL Shasta URL URL URL URL Sierra URL URL URL URL Siskiyou URL URL URL URL Solano URL URL URL URL Sonoma URL URL URL URL Stanislaus URL URL URL URL Sutter URL URL URL URL Tehama URL URL URL URL Trinity URL URL URL URL Tulare URL URL URL URL Tuolumne URL URL URL URL Ventura URL URL URL URL Yolo URL URL URL URL Yuba URL URL URL URL"},{"location":"dbt/dbt-performance/","title":"dbt performance evaluation and tuning","text":""},{"location":"dbt/dbt-performance/#considerations-when-does-performance-matter","title":"Considerations: When does performance matter?","text":"<p>In most settings, what is considered acceptable performance is relative to business needs and constraints. It's not atypical to deem the performance acceptable as long as there are no scheduling conflicts and models can run within a timeframe dictated by the frequency of the models running. In other words, if you need to run models every hour then the entire job cannot take longer than an hour to run. In general, compute costs are not so high to necessarily be worth optimizing the underlying queries but may be high enough to optimize frequency or data size.</p>"},{"location":"dbt/dbt-performance/#costs","title":"Costs","text":"<p>Although compute time is relatively cheap, it's sometimes possible with larger datasets that need to be frequently refreshed to optimize performance to save enough costs to be worth the time to optimize. In Snowflake you can easily monitor costs in the Admin/Usage section of the Snowflake UI, where you can see credits used by warehouse and role. Snowflake also provides several tables with meta information that can be used to derive exact costs for each query - an approach to this, with a ready-use-query can be found in the Select.dev blog post \"Calculating cost per query in Snowflake\"</p> <p>Typically, unless model performance is obviously very poor you are better off adjusting the frequency of runs (end users almost always over-state their desire for data freshness) or reducing data set size either by limiting what you provide or by using incremental models.</p> <p>In other words, very often the questions you should be asking are not in the category of SQL performance tuning but rather \"do we need this data to be this fresh?\" and \"do we need all this data?\".</p>"},{"location":"dbt/dbt-performance/#scheduling-conflicts","title":"Scheduling conflicts","text":"<p>Often performance issues show up in scheduling. If you are running jobs once a day it is extremely unlikely you will run into any scheduling conflicts. However, if a much higher frequency is required, it's possible for jobs to take longer than that time between runs. In this case a common first approach is to break up model runs so that things that don't need to run as frequently can run separately from models that require more frequent updating. A typical way of doing this is to either use dbt run --select or dbt tags dbt tags  to select models in groups. This is not to say performance tuning of individual queries is never worth it but that the big macro gains come more from running models less frequently and/or with less data, e.g. using filtering or incremental models.</p>"},{"location":"dbt/dbt-performance/#the-tradeoffs","title":"The tradeoffs","text":"<p>It is extremely important to balance time spent in optimizing model performance with compute costs and other concerns. If it takes you a day to optimize a model to run only a few seconds faster and save a few pennies per run, it's not likely worth the effort. Similarly, the use of incremental materialization can certainly reduce build time but introduce complexity and require a degree of monitoring to ensure integrity. See also Materialization Matters below.</p>"},{"location":"dbt/dbt-performance/#analyzing-performance-in-dbt","title":"Analyzing performance in dbt","text":"<p>With every dbt run or build several artifacts are generated in the target/ directory, including the run_results.json file. This includes detailed information on run execution and many people parse this to create dashboards to report on dbt performance and help with optimization and cost monitoring. There is an important caveat here: simply knowing how long a model took to run is important to uncover which models might need optimization, but cannot tell you anything about why they are performing poorly.</p>"},{"location":"dbt/dbt-performance/#getting-model-timing-local-development","title":"Getting model timing: Local development","text":"<p>Every time you run a model dbt outputs timing, information which you can easily use to identify non-performance models. The output will look like: <pre><code>14:16:39.438935 [info ] [Thread-4  ]: 136 of 160 OK created sql table model dbt_aerishan.JobControl_Published_CertEligActions  [SUCCESS 1 in 43.00s]\n</code></pre> This is extremely useful during development to understand potential problems with your model's performance.</p>"},{"location":"dbt/dbt-performance/#getting-model-timing-dbt-cloud","title":"Getting model timing: dbt Cloud","text":"<p>dbt Cloud has a nicer interface for finding which models in a project are running longest. Visit the Deploy &gt; Runs section of dbt Cloud. You'll see a full list of jobs and how long each took. To drill down to the model timing level click on a run name. You can expand the \"Invoke dbt build\" section under \"Run Summary\" to get a detailed summary of your run and timing for each model and test. There is also a \"Debug logs\" section for even more detail, including the exact queries run and an option to download the logs for easier viewing. Of course this is also where you go to find model and test errors and warnings!</p> <p></p> <p>For a quick visual reference of which models take up the most time in a run, click on the \"Model Timing\" tab. If you hover over a model you will be shown the specific timing.</p> <p></p>"},{"location":"dbt/dbt-performance/#getting-model-timing-snowflake","title":"Getting model timing: Snowflake","text":"<p>Snowflake has quite a lot of performance data readily available through its <code>information_schema.QUERY_HISTORY()</code> table function and several views in the Account Usage schema. This is great not only for finding expensive queries regardless of source and of course for all sorts of analytics on Snowflake usage, such as credits.</p>"},{"location":"dbt/dbt-performance/#query-history","title":"Query history","text":"<p>The Query History gives you real time data while the Account Usage is delayed. So Query History is great for analyzing your own queries in development and for current query performance in production.</p> <p>Example Query: Get top time-consuming queries for the dbt Cloud production loads <pre><code>SELECT\n    query_text, query_type, database_name, schema_name,\n    user_name, total_elapsed_time\n    FROM\n        -- query_history() is a table function\n        table (information_schema.query_history())\n    WHERE user_name = 'DBT_CLOUD_SVC_USER_PRD'\nORDER BY total_elapsed_time DESC\nLIMIT 20\n</code></pre></p> <p>As you might have guessed this also lets you search for a model on query text, so you can find specific dbt models or classes of models: <pre><code>    WHERE query_text LIKE '%stg_%'\n</code></pre></p>"},{"location":"dbt/dbt-performance/#account-usage","title":"Account usage","text":"<p>The Account Usage schema (snowflake.account_usage) has multiple views that are of interest for monitoring not just query performance and credit usage but warehouse and database usage and more. This data is delayed 45 minutes but has a much longer history.</p> <p>Example Query: Find the queries with highest total execution time this month for the dbt cloud production loads. <pre><code>SELECT query_text,\n    SUM(execution_time) / 60000 AS total_exec_time_mins,\n    SUM(credits_used_cloud_services) AS total_credits_used\nfrom snowflake.account_usage.query_history\nWHERE\n    start_time &gt;= date_trunc(month, current_date)\n    AND user_name = 'DBT_CLOUD_SVC_USER_PRD'\nGROUP BY 1\nORDER BY total_exec_time_mins DESC\nLIMIT 20\n</code></pre></p> <p>Example Query: Get Credits used by Warehouse this month <pre><code>select warehouse_name,\n       sum(credits_used) as total_credits_used\nfrom warehouse_metering_history\nwhere start_time &gt;= date_trunc(month, current_date)\ngroup by 1\norder by 2 desc;\n</code></pre></p>"},{"location":"dbt/dbt-performance/#solutions-how-to-tackle-dbt-performance-issues","title":"Solutions: How to tackle dbt performance issues","text":"<p>Now that you've identified which models might need optimization, it's time to figure out how to get them to run faster. These options are roughly in order of bang-for-buck in most situations.</p>"},{"location":"dbt/dbt-performance/#1-job-level-adjust-frequency-and-break-up-runs","title":"1. Job Level: Adjust frequency and break up runs","text":"<p>It's common for end-users to say they want the freshest data (who doesn't?) but in practice require a much lower frequency of refreshing. To gain an understanding of the real-world needs it's helpful to see the frequency with which end-users actually view reporting and to consider the time scales involved. If someone only cares about monthly results, for example, you can in theory have a 30 day frequency for model runs. It's also quite common to have parts of the data be relatively static, and only need to be refreshed occasionally whereas other parts of the data might change much more often. An easy way to break up model runs is by using dbt tags. <pre><code>models:\n    foo_model:\n        +tags:\n            - \"daily\"\n            - \"staging\"\n    bar_model:\n        +tags: \"hourly\"\n    baz_)_model:\n        +tags: \"weekly\"\n</code></pre></p> <p>In this case you can run certain models with: <code>dbt run --select tag:daily</code> Of course this works in dbt Cloud as well!</p> <p>For more information refer to the dbt tags documentation.</p>"},{"location":"dbt/dbt-performance/#2-model-level-materialization-matters","title":"2. Model-level: Materialization matters","text":"<p>For a good comparison of materialization options and their trade-offs see the Materialization Best Practices section of the dbt docs.</p> <p>Views: Are a trade-off between build performance and read/reporting performance. In cases where you are using a BI tool, you should almost always use table materializations unless data storage size is an issue or refresh frequency is so high that cost or scheduling conflicts become a problem. In cases where performance at time of reporting is not an issue (say, you are generating an aggregated report on a monthly basis) then views can be a great way to cut run time. Another case where views can be a good option is with staging data of relatively small size, where your queries are relatively light-weight and you want to ensure fresh data without having to configure separate runs for those models.</p> <p>Incremental Models: For very large data sets, it can be essential to use incremental models. For this to work, you need some means of filtering records from the source table, typically using a timestamp. You then add a conditional block into your model to only select new records unless you're doing a full refresh. It's worth noting that incremental models can be tricky to get right and you will often want to implement some additional data integrity testing to ensure data is fresh and complete. For a more detailed discussion of Incremental Models, see Incremental models in-depth</p> <p>An example in our current projects is the CalHR Ecos model stg_CertEligibles. This query takes over three minutes to run and no wonder - it generates 858 million rows! This is clearly a case where we should ask if we need all of that data or can it be filtered in some way and if the answer is yes, then we should consider using an incremental materialization.</p>"},{"location":"dbt/dbt-performance/#4-query-level-optimizing-queries","title":"4. Query-level: Optimizing queries","text":"<p>So many books have been written on this subject! The good news is that most tools we use provide excellent resources for analyzing query performance.</p>"},{"location":"dbt/dbt-performance/#write-or-read","title":"Write or read?","text":"<p>Because models are often created using a CREATE TABLE... SELECT statement you need to separate out read from write performance to understand if the issue is that your original query is slow or you are simply moving a lot of data and it takes time. It's worth saying that the chances are good that if you are moving a lot of data you are also querying a lot of data and in fact both read and write may be very time consuming but this is not a given -- if you are doing lots of joins on big data sets along with aggregations that output a small number of rows, then probably your model performance is read-bound. If this is the case the first question you should probably ask is can you break up that model into smaller chunks using staging and intermediate models.</p> <p>A good way to get a sense of read vs write performance is to do one or more of: 1. Simply know the number of rows generated by the model (for some database dbt will output this in the output above). If you are creating tables with millions of rows you should probably consider an incremental model or reassess if you can filter and narrow your data somehow. 2. Use your database's query profiler, if available, to separate out what part of the execution is taking the most amount of time. In Snowflake for example, you can use the query profile to easily determine whether a query is rebound or write down and also determine where exactly other performance issues may lie. A CREATE TABLE with a simple select, for example, will show that the majority of time is spent in the CreateTableAsSelect node and only a fraction of the time in the Result node. Be careful if you are comparing queries across runs - most databases use caching and this will of course affect your results (see Caching Notes below). 3. Switch the materialization to view. Typically a view will take a fraction of the time to generate, and if that's the case you know your model is slow in writes. 4. Run the query separately in the database without the CREATE TABLE part. When you do this you can typically assess the execution plan</p>"},{"location":"dbt/dbt-performance/#snowflake-query-profile","title":"Snowflake query profile","text":"<p>You can easily pull up the query profile for any query that has been run in Snowflake either from a worksheet or from the query history page. This includes queries run from dbt Cloud! This profile is essential in understanding the elements of your query that are most costly in terms of time, and which might be improved through optimization. Refer to the Analyzing Queries Using Query Profile page in the Snowflake Documentation for complete information including common problems and their solutions.</p>"},{"location":"dbt/dbt-performance/#bigquery-query-plan","title":"BigQuery query plan","text":"<p>Big Query offers similar query execution profiling in the Google Cloud Console. See Query plan and timeline as well as Big Query's Introduction to optimizing query performance</p>"},{"location":"dbt/dbt-performance/#caching-notes","title":"Caching notes","text":"<p>Most databases use some type of caching which needs to be turned off to properly test performance. Snowflake uses both a results cache and a disk cache, but only one can be turned off with a session variable: <pre><code>alter session set use_cached_result = false;\n</code></pre> See this in-depth discussion for more details: Deep Dive on Snowflake Caching A general workaround (other than to shutdown and restart the warehouse) is to use slightly different result sets which do the same operations and return the same number of rows.</p>"},{"location":"dbt/dbt-performance/#local-development-tips","title":"Local development tips","text":"<ol> <li>Use Tags or dbt run --select to limit what you are building</li> <li>Use --select state:modified+ result:error+ to limit runs</li> <li>You can also include limits on data when working with a development target, e.g. <code>{% if target.name = 'dev' %}     LIMIT 500 {% endif %}</code></li> </ol>"},{"location":"dbt/dbt/","title":"dbt on the Data Services and Engineering team","text":""},{"location":"dbt/dbt/#architecture","title":"Architecture","text":"<p>We broadly follow the architecture described in this dbt blog post for our Snowflake dbt project.</p> <p>It is described in more detail in our Snowflake docs.</p>"},{"location":"dbt/dbt/#naming-conventions","title":"Naming conventions","text":"<p>Models in a data warehouse do not follow the same naming conventions as raw cloud resources, as their most frequent consumers are analytics engineers and data analysts.</p> <p>The following conventions are used where appropriate:</p> <p>Dimension tables are prefixed with <code>dim_</code>.</p> <p>Fact tables are prefixed with <code>fct_</code>.</p> <p>Staging tables are prefixed with <code>stg_</code>.</p> <p>Intermediate tables are prefixed with <code>int_</code>.</p> <p>We may adopt additional conventions for denoting aggregations, column data types, etc. in the future. If during the course of a project's model development we determine that simpler human-readable names work better for our partners or downstream consumers, we may drop the above prefixing conventions.</p>"},{"location":"dbt/dbt/#custom-schema-names","title":"Custom schema names","text":"<p>dbt's default method for generating custom schema names works well for a single-database setup:</p> <ul> <li>It allows development work to occur in a separate schema from production models.</li> <li>It allows analytics engineers to develop side-by-side without stepping on each others toes.</li> </ul> <p>A downside of the default is that production models all get a prefix, which may not be an ideal naming convention for end-users.</p> <p>Because our architecture separates development and production databases, and has strict permissions protecting the <code>RAW</code> database, there is less danger of breaking production models. So we use our own custom schema name following a modified approach from the GitLab Data Team.</p> <p>In production, each schema is just the custom schema name without any prefix. In non-production environments the default is used, where analytics engineers get the custom schema name prefixed with their target schema name (i.e. <code>dbt_username_schemaname</code>), and CI runs get the custom schema name prefixed with a CI job name.</p> <p>This approach may be reevaluated as the project matures.</p>"},{"location":"infra/architecture/","title":"Project architecture","text":"<p>This project is organized around centralizing data in a Snowflake cloud data warehouse. Heterogeneous data sources are loaded into raw databases using custom Python scripts. These datasets are then transformed into analysis-ready marts using dbt.</p> <p>We follow an adapted version of the project architecture described in this dbt blog post for our Snowflake dbt project.</p> <p>It is described in some detail in our Snowflake docs as well.</p> <pre><code>flowchart TB\nsubgraph External Data Source\n    direction TB\n    DS1[(\\nData Source 1)]\n    DS2[(\\nData Source 2)]\n    DS3[(\\nData Source 3)]\n  end\n  PY&gt;Python scripts]\n  F&gt;Fivetran]\nsubgraph AWS\n  direction TB\n  A[\\AWS S3 Bucket/]\n  subgraph ODI Snowflake\n    subgraph PRD\n        direction TB\n        RP[(RAW_PRD)]\n        TP[(TRANSFORM_PRD)]\n        AP[(ANALYTICS_PRD)]\n    end\n    subgraph DEV\n        direction TB\n        RD[(RAW_DEV)]\n        TD[(TRANSFORM_DEV)]\n        AD[(ANALYTICS_DEV)]\n    end\n  end\nend\nP{{Reporting tool e.g. PowerBI or Tableau}}\n\nF -- LOADER_PRD --&gt; A\nA -- Data Pipelines --&gt; RP\nRD -- TRANSFORMER_DEV --&gt; TD\nTD -- TRANSFORMER_DEV --&gt; AD\nRP -- TRANSFORMER_PRD --&gt; TP\nTP -- TRANSFORMER_PRD --&gt; AP\nRP -- TRANSFORMER_DEV --&gt; TD\nAD -- REPORTER_DEV --&gt; P\nAP -- REPORTER_PRD --&gt; P\nDS1 --&gt; PY\nDS2 --&gt; F\nDS3 --&gt; F\nPY --&gt; RP\nPY --&gt; RD\nF --&gt; RP\nF --&gt; RD</code></pre>"},{"location":"infra/architecture/#snowflake-architecture","title":"Snowflake architecture","text":"<p>There are two environments set up for this project, development and production. Resources in the development environment are suffixed with <code>DEV</code>, and resources in the production environment are suffixed with <code>PRD</code>.</p> <p>Most of the time, developers will be working in the development environment. Once your feature branches are merged to <code>main</code>, they will be used in the production environment.</p> <p>What follows is a brief description of the most important Snowflake resources in the dev and production environments and how developers are likely to interact with them.</p>"},{"location":"infra/architecture/#six-databases","title":"Six databases","text":"<p>We have six primary databases in our project:</p> <p>Where our Source data lives</p> <ul> <li><code>RAW_DEV</code>: Dev space for loading new source data.</li> <li><code>RAW_PRD</code>: Landing database for production source data.</li> </ul> <p>Where data from our Staging and Intermediate models lives</p> <ul> <li><code>TRANSFORM_DEV</code>: Dev space for staging/intermediate models. This is where most of your dbt work is!</li> <li><code>TRANSFORM_PRD</code>: Prod space for models. This is what builds in the nightly job.</li> </ul> <p>Where data from our Marts models lives</p> <ul> <li><code>ANALYTICS_DEV</code>: Dev space for mart models. Use this when developing a model for a new dashboard or report!</li> <li><code>ANALYTICS_PRD</code>: Prod space for mart models. Point production dashboards and reports to this database.</li> </ul>"},{"location":"infra/architecture/#six-warehouse-groups","title":"Six warehouse groups","text":"<p>There are six warehouse groups for processing data in the databases, corresponding to the primary purposes of the above databases. They are available in a few different sizes, depending upon the needs of the the data processing job, X-small denoted by (<code>XS</code>), Small (denoted by <code>S</code>), Medium (denoted by <code>M</code>), Large denoted by (<code>L</code>), X-Large denoted by (<code>XL</code>), 2X-Large (denoted by <code>2XL</code>), 3X-Large (denoted by <code>3XL</code>) and 4X-Large (denoted by <code>4XL</code>). Most jobs on small data should use the relevant X-small warehouse.</p> <p>Following is a general guideline from Snowflake for choosing a warehouse size.</p> <p>X-Small: Good for small tasks and experimenting. Small: Suitable for single-user workloads and development. Medium: Handles moderate concurrency and data volumes. Large: Manages larger queries and higher concurrency. X-Large: Powerful for demanding workloads and data-intensive operations. 2X-Large: Double the capacity of X-Large. 3X-Large: Triple the capacity of X-Large. 4X-Large: Quadruple the capacity of X-Large.</p> <ol> <li><code>LOADING_{size}_DEV</code>: This warehouse is for loading data to <code>RAW_DEV</code>. It is used for testing new data loading scripts.</li> <li><code>TRANSFORMING_{size}_DEV</code>: This warehouse is for transforming data in <code>TRANSFORM_DEV</code> and <code>ANALYTICS_DEV</code>. Most dbt developers will use this warehouse for daily work.</li> <li><code>REPORTING_{size}_DEV</code>: This warehouse is for testing dashboards.</li> <li><code>LOADING_{size}_PRD</code>: This warehouse is for loading data to <code>RAW_PRD</code>. It is used for production data loading scripts.</li> <li><code>TRANSFORMING_{size}_PRD</code>: This warehouse is for transforming data in <code>TRANSFORM_PRD</code> and <code>ANALYTICS_PRD</code>. This warehouse is used for the nightly builds.</li> <li><code>REPORTING_{size}_PRD</code>: This warehouse is for production dashboards.</li> </ol>"},{"location":"infra/architecture/#six-roles","title":"Six roles","text":"<p>There are six primary functional roles:</p> <ol> <li><code>LOADER_DEV</code>: Dev role for loading data to the <code>RAW_DEV</code> database. This is assumed when developing new data loading scripts.</li> <li> <p><code>LOADER_PRD</code>: Prod role for loading data to the <code>RAW_PRD</code> database. This is assumed by data loading scripts.</p> </li> <li> <p><code>TRANSFORMER_DEV</code>: Dev role for transforming data. This is you! Models built with this role get written to the <code>TRANSFORM_DEV</code> or <code>ANALYTICS_DEV</code> databases. CI robots also use this role to run checks and tests on PRs before they are merged to main.</p> </li> <li> <p><code>TRANSFORMER_PRD</code>: Prod role for transforming data. This is assumed by the nightly build job and writes data to the <code>TRANSFORM_PRD</code> or <code>ANALYTICS_PRD</code> databases.</p> </li> <li> <p><code>REPORTER_DEV</code>: Dev role for reading marts. Use this when developing new dashboards. This role can read models in the <code>ANALYTICS_DEV</code> database.</p> </li> <li><code>REPORTER_PRD</code>: Prod role for reading marts. This is for users and service accounts using production dashboards. This role can read models in the <code>ANALYTICS_PRD</code> database.</li> </ol>"},{"location":"infra/architecture/#reporting-and-analysis","title":"Reporting and analysis","text":"<p>The most prominent consumers of the data products from this project are PowerBI and Tableau dashboards and the CalInnovate team.</p>"},{"location":"infra/architecture/#custom-schema-names","title":"Custom schema names","text":"<p>dbt's default method for generating custom schema names works well for a single-database setup:</p> <ul> <li>It allows development work to occur in a separate schema from production models.</li> <li>It allows analytics engineers to develop side-by-side without stepping on each other's toes.</li> </ul> <p>A downside of the default is that production models all get a prefix, which may not be an ideal naming convention for end-users.</p> <p>Because our architecture separates development and production databases, and has strict permissions protecting the <code>RAW_PRD</code> database, there is less danger of breaking production models. So we use our own custom schema name modified from the approach of the GitLab Data Team.</p> <p>In production, each schema is just the custom schema name without any prefix. In non-production environments, dbt developers use their own custom schema based on their name: <code>dbt_username</code>.</p>"},{"location":"infra/architecture/#examples","title":"Examples","text":""},{"location":"infra/architecture/#user-personae","title":"User personae","text":"<p>To make the preceding more concrete, let's consider the six databases, <code>RAW</code>, <code>TRANSFORM</code>, and <code>ANALYTICS</code>, for both <code>DEV</code> and <code>PRD</code>:</p> <p></p> <p>If you are a developer, you are doing most of your work in <code>TRANSFORM_DEV</code> and <code>ANALYTICS_DEV</code>, assuming the role <code>TRANSFORMER_DEV</code>. However, you also have the ability to select the production data from <code>RAW_PRD</code> for your development. So your data access looks like the following:</p> <p></p> <p>Now let's consider the nightly production build. This service account builds the production models in <code>TRANSFORM_PRD</code> and <code>ANALYTICS_PRD</code> based on the raw data in <code>RAW_PRD</code>. The development environment effectively doesn't exist to this account, and data access looks like the following:</p> <p></p> <p>Finally, let's consider an external consumer of a mart from PowerBI. This user has no access to any of the raw or intermediate models (which might contain sensitive data!). To them, the whole rest of the architecture doesn't exist, and they can only see the marts in <code>ANALYTICS_PRD</code>:</p> <p></p>"},{"location":"infra/architecture/#scenario-adding-a-new-data-source","title":"Scenario: adding a new data source","text":"<ol> <li>Write a new Python script (or configure an equivalent loading tool) for loading the data to Snowflake.</li> <li>Assume the <code>LOADER_DEV</code> role and load the data into the <code>RAW_DEV</code> database.</li> <li>Verify that the data was loaded and looks correct in Snowflake.</li> <li>Schedule the Python script to run using the <code>LOADER_PRD</code> role and the <code>RAW_PRD</code> database. These data are now ready for dbt modeling.</li> <li>Once the data is loaded to the <code>RAW_PRD</code> database, drop the data from the <code>RAW_DEV</code> database, it\u2019s not needed anymore.</li> </ol>"},{"location":"infra/architecture/#scenario-creating-a-new-dashboard","title":"Scenario: creating a new dashboard","text":"<ol> <li>Create a branch and develop your new marts table (or modify an existing one) in <code>ANALYTICS_DEV</code> using your normal <code>TRANSFORMER_DEV</code> role.</li> <li>Assume the <code>REPORTER_DEV</code> role with PowerBI and point it to your mart in <code>ANALYTICS_DEV</code>.</li> <li>Once you are happy with your mart and dashboard, merge your branch to main.</li> <li>Once the nightly job is done, your new mart will be built in <code>ANALYTICS_PRD</code>.</li> <li>Assume the <code>REPORTER_PRD</code> role and point your PowerBI dashboard to the production mart.</li> </ol>"},{"location":"infra/cloud-infrastructure/","title":"Cloud infrastructure","text":"<p>The DSE team uses Terraform to manage cloud infrastructure. Our stack includes:</p> <ul> <li>An AWS Batch environment for running arbitrary containerized jobs</li> <li>A Managed Workflows on Apache Airflow environment for orchestrating jobs</li> <li>A VPC and subnets for the above</li> <li>An ECR repository for hosting Docker images storing code and libraries for jobs</li> <li>A bot user for running AWS operations in GitHub Actions</li> <li>An S3 scratch bucket</li> </ul>"},{"location":"infra/cloud-infrastructure/#architecture","title":"Architecture","text":"<pre><code>flowchart TD\n  subgraph AWS\n    J[GitHub CD\\nbot user]\n    G[Artifact in S3]\n    subgraph VPC\n      subgraph Managed Airflow\n        K1[Scheduler]\n        K2[Worker]\n        K3[Webserver]\n      end\n      F[AWS Batch Job\\n on Fargate]\n    end\n    E[AWS ECR Docker\\nRepository]\n  end\n  subgraph GitHub\n    A[Code Repository]\n  end\n  E --&gt; F\n  A -- Code quality check\\n GitHub action --&gt; A\n  A -- Job submission\\nvia GitHub Action --&gt; F\n  A -- Docker build \\nGitHub Action --&gt; E\n  A --&gt; H[CalData\\nadministrative\\nuser]\n  H -- Terraform -----&gt; AWS\n  K2 -- Job submission\\nvia Airflow --&gt; F\n  K1 &lt;--&gt; K2\n  K3 &lt;--&gt; K1\n  K3 &lt;--&gt; K2\n  F --&gt; G\n  J -- Bot Credentials --&gt; A</code></pre>"},{"location":"infra/snowflake/","title":"Snowflake","text":"<p>We use Snowflake as our primary data warehouse.</p>"},{"location":"infra/snowflake/#architecture","title":"Architecture","text":"<p>The setup of our account is adapted from the approach described in this dbt blog post, which we summarize here.</p> <p>Note</p> <p>We have development and production environments, which we denote with <code>_DEV</code> and <code>_PRD</code> suffixes on Snowflake objects. For that reason, some of the names here are not exactly what exists in our deployment, but are given in the un-suffixed form for clarity.</p> <pre><code>flowchart LR\n  Airflow((Airflow))\n  Fivetran((Fivetran))\n  subgraph RAW\n    direction LR\n    A[(SCHEMA A)]\n    B[(SCHEMA A)]\n    C[(SCHEMA A)]\n  end\n  DBT1((dbt))\n  subgraph TRANSFORM\n    direction LR\n    D[(SCHEMA A)]\n    E[(SCHEMA A)]\n    F[(SCHEMA A)]\n  end\n  DBT2((dbt))\n  subgraph ANALYTICS\n    direction LR\n    G[(SCHEMA A)]\n    H[(SCHEMA A)]\n    I[(SCHEMA A)]\n  end\n  PowerBI\n  Tableau\n  Python\n  R\n\n  Airflow -- LOADER --&gt; RAW\n  Fivetran -- LOADER --&gt; RAW\n  RAW -- TRANSFORMER --&gt; DBT1\n  DBT1 -- TRANSFORMER --&gt; TRANSFORM\n  TRANSFORM -- TRANSFORMER --&gt; DBT2\n  DBT2 -- TRANSFORMER --&gt; ANALYTICS\n  ANALYTICS -- REPORTER --&gt; Tableau\n  ANALYTICS -- REPORTER --&gt; Python\n  ANALYTICS -- REPORTER --&gt; R\n  ANALYTICS -- REPORTER --&gt; PowerBI</code></pre>"},{"location":"infra/snowflake/#three-databases","title":"Three databases","text":"<p>We have three primary databases in our account:</p> <ol> <li><code>RAW_{env}</code>: This holds raw data loaded from tools like Fivetran or Airflow. It is strictly permissioned, and only loader tools should have the ability to load or change data.</li> <li><code>TRANSFORM_{env}</code>: This holds intermediate results, including staging data, joined datasets, and aggregations. It is the primary database where development/analytics engineering happens.</li> <li><code>ANALYTICS_{env}</code>: This holds analysis/BI-ready datasets. This is the \"marts\" database.</li> </ol>"},{"location":"infra/snowflake/#three-warehouse-groups","title":"Three warehouse groups","text":"<p>There are warehouse groups for processing data in the databases, corresponding to the primary purposes of the above databases. They are available in a few different sizes, depending upon the needs of the the data processing job, X-small denoted by (<code>XS</code>), Small (denoted by <code>S</code>), Medium (denoted by <code>M</code>), Large denoted by (<code>L</code>), X-Large denoted by (<code>XL</code>), 2X-Large (denoted by <code>2XL</code>), 3X-Large (denoted by <code>3XL</code>) and 4X-Large (denoted by <code>4XL</code>). Most jobs on small data should use the relevant X-small warehouse.</p> <p>Following is a general guideline from Snowflake for choosing a warehouse size.</p> <p>X-Small: Good for small tasks and experimenting. Small: Suitable for single-user workloads and development. Medium: Handles moderate concurrency and data volumes. Large: Manages larger queries and higher concurrency. X-Large: Powerful for demanding workloads and data-intensive operations. 2X-Large: Double the capacity of X-Large. 3X-Large: Triple the capacity of X-Large. 4X-Large: Quadruple the capacity of X-Large.</p> <ol> <li><code>LOADING_{size}_{env}</code>: This warehouse is for loading data to <code>RAW</code>.</li> <li><code>TRANSFORMING_{size}_{env}</code>: This warehouse is for transforming data in <code>TRANSFORM</code> and <code>ANALYTICS</code>.</li> <li><code>REPORTING_{size}_{env}</code>: This warehouse is the role for BI tools and other end-users of the data.</li> </ol>"},{"location":"infra/snowflake/#four-roles","title":"Four roles","text":"<p>There are four primary functional roles:</p> <ol> <li><code>LOADER_{env}</code>: This role is for tooling like Fivetran or Airflow to load raw data into the <code>RAW</code> database.</li> <li><code>TRANSFORMER_{env}</code>: This is the analytics engineer/dbt role, for transforming raw data into something analysis-ready. It has read/write/control access to both <code>TRANSFORM</code> and <code>ANALYTICS</code>, and read access to <code>RAW</code>.</li> <li><code>REPORTER_{env}</code>: This role read access to <code>ANALYTICS</code>, and is intended for BI tools and other end-users of the data.</li> <li><code>READER_{env}</code>: This role has read access to all three databases, and is intended for CI service accounts to generate documentation.</li> </ol>"},{"location":"infra/snowflake/#access-roles-vs-functional-roles","title":"Access Roles vs Functional Roles","text":"<p>We create a two layer role hierarchy according to Snowflake's guidelines:</p> <ul> <li>Access Roles are roles giving a specific access type (read, write, or control) to a specific database object, e.g., \"read access on <code>RAW</code>\".</li> <li>Functional Roles represent specific user personae like \"developer\" or \"analyst\" or \"administrator\". Functional roles are built by being granted a set of Access Roles.</li> </ul> <p>There is no technical difference between access roles and functional roles in Snowflake. The difference lies in the semantics and hierarchy that we impose upon them.</p>"},{"location":"infra/snowflake/#security-policies","title":"Security Policies","text":"<p>Our security policies and norms for Snowflake are following the best practices laid out in this article, these overview docs, and conversations had with our Snowflake representatives.</p>"},{"location":"infra/snowflake/#use-federated-single-sign-on-sso-and-system-for-cross-domain-identity-management-scim-for-human-users","title":"Use Federated Single Sign-On (SSO) and System for Cross-domain Identity Management (SCIM) for human users","text":"<p>Most State departments will have a federated identity provider for SSO and SCIM. At the Office of Data and Innovation, we use Okta. Many State departments use Active Directory.</p> <p>Most human users should have their account lifecycle managed through SCIM, and should log in via SSO.</p> <p>Using SCIM with Snowflake requires creating an authorization token for the account. This token should be stored in DSE's shared 1Password vault, and needs to be manually rotated every six months.</p>"},{"location":"infra/snowflake/#enable-multi-factor-authentication-mfa-for-users","title":"Enable multi-factor authentication (MFA) for users","text":"<p>Users, especially those with elevated permissions, should have multi-factor authentication enabled for their accounts. In some cases, this may be provided by their SSO identity provider, and in some cases this may use the built-in Snowflake MFA using Duo.</p>"},{"location":"infra/snowflake/#use-auto-sign-out-for-snowflake-sessions","title":"Use auto-sign-out for Snowflake sessions","text":"<p>Ensure that <code>CLIENT_SESSION_KEEP_ALIVE</code> is set to <code>FALSE</code> in the account. This means that unattended browser windows will automatically sign out after a set amount of time (defaulting to one hour).</p>"},{"location":"infra/snowflake/#follow-the-principle-of-least-privilege","title":"Follow the principle of least-privilege","text":"<p>In general, users and roles should be assigned permissions according to the Principle of Least Privilege, which states that they should have sufficient privileges to perform legitimate work, and no more. This reduces security risks should a particular user or role become compromised.</p>"},{"location":"infra/snowflake/#regularly-review-users-with-elevated-privileges","title":"Regularly review users with elevated privileges","text":"<p>Users with access to elevated privileges (especially the <code>ACCOUNTADMIN</code>, <code>SECURITYADMIN</code>, and <code>SYSADMIN</code> roles) should be regularly reviewed by account administrators.</p>"},{"location":"infra/snowflake/#snowflake-terraform-configuration","title":"Snowflake Terraform configuration","text":"<p>We provision our Snowflake account using terraform.</p> <p>The infrastructure is organized into three modules, two lower level ones and one application-level one:</p> <ul> <li> <p>database: A module which creates a Snowflake database and access roles for it.</p> </li> <li> <p>warehouse: A module which creates a Snowflake warehouse and access roles for it.</p> </li> <li> <p>elt: A module which creates several database and warehouse objects and implements the above architecture for them.</p> </li> </ul> <p>The elt module is then consumed by two different terraform deployments:</p> <ul> <li>dev for dev infrastructure</li> <li>prd for production infrastructure.</li> </ul> <p>The elt module has the following configuration:</p>"},{"location":"infra/snowflake/#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 snowflake ~&gt; 0.88"},{"location":"infra/snowflake/#providers","title":"Providers","text":"Name Version snowflake ~&gt; 0.88 snowflake.accountadmin ~&gt; 0.88 snowflake.useradmin ~&gt; 0.88"},{"location":"infra/snowflake/#modules","title":"Modules","text":"Name Source Version analytics ../database n/a loading ../warehouse n/a logging ../warehouse n/a raw ../database n/a reporting ../warehouse n/a transform ../database n/a transforming ../warehouse n/a"},{"location":"infra/snowflake/#resources","title":"Resources","text":"Name Type snowflake_grant_account_role.analytics_r_to_reader resource snowflake_grant_account_role.analytics_r_to_reporter resource snowflake_grant_account_role.analytics_rwc_to_transformer resource snowflake_grant_account_role.loader_to_airflow resource snowflake_grant_account_role.loader_to_fivetran resource snowflake_grant_account_role.loader_to_sysadmin resource snowflake_grant_account_role.loading_to_loader resource snowflake_grant_account_role.logger_to_accountadmin resource snowflake_grant_account_role.logger_to_sentinel resource snowflake_grant_account_role.logging_to_logger resource snowflake_grant_account_role.raw_r_to_reader resource snowflake_grant_account_role.raw_r_to_transformer resource snowflake_grant_account_role.raw_rwc_to_loader resource snowflake_grant_account_role.reader_to_github_ci resource snowflake_grant_account_role.reader_to_sysadmin resource snowflake_grant_account_role.reporter_to_sysadmin resource snowflake_grant_account_role.reporting_to_reader resource snowflake_grant_account_role.reporting_to_reporter resource snowflake_grant_account_role.transform_r_to_reader resource snowflake_grant_account_role.transform_rwc_to_transformer resource snowflake_grant_account_role.transformer_to_dbt resource snowflake_grant_account_role.transformer_to_sysadmin resource snowflake_grant_account_role.transforming_to_transformer resource snowflake_grant_privileges_to_account_role.imported_privileges_to_logger resource snowflake_role.loader resource snowflake_role.logger resource snowflake_role.reader resource snowflake_role.reporter resource snowflake_role.transformer resource snowflake_user.airflow resource snowflake_user.dbt resource snowflake_user.fivetran resource snowflake_user.github_ci resource snowflake_user.sentinel resource"},{"location":"infra/snowflake/#inputs","title":"Inputs","text":"Name Description Type Default Required environment Environment suffix <code>string</code> n/a yes"},{"location":"infra/snowflake/#outputs","title":"Outputs","text":"<p>No outputs.</p>"},{"location":"learning/cloud-data-warehouses/","title":"Cloud data warehouses","text":""},{"location":"learning/cloud-data-warehouses/#what-is-a-cloud-data-warehouse","title":"What is a cloud data warehouse?","text":"<p>Cloud data warehouses (CDWs) are databases which are hosted in the cloud, and are typically optimized around analytical queries like aggregations and window functions, rather than the typical transactional queries that might support a traditional application. Examples of popular cloud data warehouses include Google BigQuery, Amazon Redshift, and Snowflake.</p> <p>Cloud data warehouses typically have a few advantages over traditional transactional databases for analytical workflows, including:</p> <ul> <li>They are usually managed services, meaning you don't have to provision and maintain servers.</li> <li>They can scale to truly massive data.</li> </ul> <p>By having a solid understanding of how cloud data warehouses work, you can construct fast, efficient queries and avoid surprise costs.</p>"},{"location":"learning/cloud-data-warehouses/#usage-based-pricing","title":"Usage-based pricing","text":"<p>With most on-premise transactional warehouses, costs scale with the number of server instances you buy and run. These servers usually are always-on and power various applications with high availability. In a traditional transactional warehouse both compute power and storage are associated with the same logical machine.</p> <p>Cloud data warehouses typically have a different pricing model: they decouple storage and compute and charge based on your query usage. Google BigQuery charges based on the amount of data your queries scan. Snowflake charges based on the amount of compute resources needed to execute your queries. There are also costs associated with data storage, but those are usually small compared to compute. Though these two models are slightly different, they both lead to a similar take-home lesson: by being careful with how data are laid out and accessed, you can significantly reduce both execution time and cost for your cloud data warehouses.</p>"},{"location":"learning/cloud-data-warehouses/#data-layout","title":"Data layout","text":"<p>Most cloud data warehouses use columnar storage for their data. This means that data for each column of a table are stored sequentially in object storage (this is in contrast to transactional databases which usually store each row, or record, sequentially in storage). This BigQuery blog post goes into a bit more detail.</p> <p></p> <p>There are a number of consequences of using columnar storage:</p> <ul> <li>You can read in columns separately from each other. So if your query only needs to look at one column of a several-hundred column table, it can do that without incurring the cost of loading and processing all of the other columns.</li> <li>Because the values in a column are located near each other in device storage, it is much faster to read them all at once for analytical queries like aggregations or window functions. In row-based storage, there is much more jumping around to different parts of memory.</li> <li>Having values of the same data type stored sequentially allows for much more efficient serialization and compression of the data at rest.</li> </ul> <p>In addition to columnar storage, cloud data warehouses also usually divide tables row-wise into chunks called partitions. Different warehouses choose different sizing strategies for partitions, but they are typically from a few to a few hundred megabytes. Having separate logical partitions in a table allows the compute resources to process the partitions independently of each other in parallel. This massively parallel processing capability is a large part of what makes cloud data warehouses scalable. When designing your tables, you can often set partitioning strategies or clustering keys for the table. This tells the cloud data warehouse to store rows with similar values for those keys within the same partitions. A well-partitioned table can enable queries to only read from the partitions that it needs, and ignore the rest.</p>"},{"location":"learning/cloud-data-warehouses/#constructing-queries-for-cloud-data-warehouses","title":"Constructing queries for cloud data warehouses","text":"<p>With the above understanding of how cloud data warehouses store and process data, we can write down a set of recommendations for how to construct efficient queries for large tables stored within them:</p> <ul> <li>Only SELECT the columns you need. Columnar storage allows you to ignore the columns you don't need, and avoid the cost of reading it in. SELECT * can get expensive!</li> <li>If the table has a natural ordering, consider setting a partitioning or clustering key. For example, if the data in the table consists of events with an associated timestamp, you might want to partition according to that timestamp. Then events with similar times would be stored near each other in the same or adjacent partitions, and queries selecting for a particular date range would have to scan fewer partitions.</li> <li>If the table has a partitioning or clustering key already set, try to filter based on that in your queries. This can greatly reduce the amount of data you need to scan.</li> <li>Filter early in complex queries, rather than at the end. If you have complex, multi-stage queries, filtering down to the subset of interest at the outset can avoid the need to process unnecessary data and then throw it away later in the query.</li> </ul> <p>Note</p> <p>For people coming from transactional databases, the considerations about partitioning and clustering may seem reminiscent of indexes. Cloud data warehouses usually don't have traditional indexes, but partitioning and clustering keys fill approximately the same role, tailored to the distributed compute model.</p>"},{"location":"learning/cloud-data-warehouses/#primary-keys-and-constraints","title":"Primary keys and constraints","text":"<p>A central feature of cloud data warehouses is that storage is separate from compute, and data can be processed in parallel by distributed compute resources. The less communication that needs to happen between these distributed compute resources, the faster they can work. For this reason, most cloud data warehouses do not support primary keys, foreign keys, or other constraints.</p> <p>For example: if we have a foreign key constraint set on a table and insert a new record, we would have to scan every single row of the parent table to see if the referenced row exists and is unique. If the table is large and partitioned, this could mean spinning up a large amount of compute resources, just to insert a single row. So rather than supporting constraints with horrible performance characteristics, cloud data warehouses just don't do it. This can be surprising to some people, since they often still include the syntax for constraints for SQL standard compatibility (see the Snowflake docs on constraints).</p> <p>Note</p> <p>One exception to the above is <code>NOT NULL</code> constraints, which can be done cheaply since they don't require information from other tables or partitions to be enforced.</p>"},{"location":"learning/cloud-data-warehouses/#interactive-exercise","title":"Interactive Exercise","text":"<p>This exercise is intended to be done live with collaborators. It should read fine, but will be more impactful if we set up a lab setting! We'll be querying Google Analytics 4 data stored in BigQuery. The dataset in question consists of (at the time of this writing) about six months of user event data collected from websites under ca.gov domain. It has over 500 million events in about 400 gigabytes of storage. The table is partitioned by event date, so all events on the same day get put in the same partition.</p>"},{"location":"learning/cloud-data-warehouses/#initial-query","title":"Initial query","text":"<p>Suppose we want to analyze the breakdown of the different web browsers used to access state sites so we can understand which browsers are the highest priority to support. We expect this to be a moving target as different browsers become more or less popular, so we'll try to restrict our analysis to the month of January, 2023. Fortunately, the dataset has a timestamp column, so we can try to filter based on that column:</p> <pre><code>SELECT * from `analytics_staging.base_ga4__events`\nWHERE event_timestamp &gt;= unix_seconds(TIMESTAMP '2023-01-01')\nAND event_timestamp &lt;= unix_seconds(TIMESTAMP '2023-01-31')\n</code></pre> <p></p> <p>Yikes! This query scans the whole 400 GB dataset. Based on Google's approximately $5/TB charge, this costs about $2, and if it were a query we were running many times a day, it could easily start costing thousands of dollars per year.</p>"},{"location":"learning/cloud-data-warehouses/#take-advantage-of-column-pruning","title":"Take advantage of column pruning","text":"<p>You'll note that we are doing a <code>SELECT *</code> query, but if we're interested in browser usage, we really only need that column. So let's just <code>SELECT</code> that column:</p> <pre><code>SELECT device_browser from `analytics_staging.base_ga4__events`\nWHERE event_timestamp &gt;= unix_seconds(TIMESTAMP '2023-01-01')\nAND event_timestamp &lt;= unix_seconds(TIMESTAMP '2023-01-31')\n</code></pre> <p></p> <p>By just selecting the column we wanted, we avoided loading a lot of unnecessary data, and now we are only scanning ~4 GB of data, reducing the charge by 99%!</p>"},{"location":"learning/cloud-data-warehouses/#take-advantage-of-partition-pruning","title":"Take advantage of partition pruning","text":"<p>In the above query we are filtering based on the <code>event_timestamp</code> field. However, the dataset actually has two different time-like fields, and it is partitioned based on the other one! The query planner is not smart enough to know that both fields contain similar information, and it is therefore not able to infer that we don't need to scan every partition to get the data within the January time window. Let's fix that by re-working the query to use the partitioned-by <code>DATE</code> field:</p> <pre><code>SELECT device_browser from `analytics_staging.base_ga4__events`\nWHERE event_date_dt &gt;= '2023-01-01'\nAND event_date_dt &lt;= '2023-01-31'\n</code></pre> <p></p> <p>By using the field by which the table is partitioned in our filter, we reduced the data scanned by another factor of ~5 (as discussed above, this is analogous to using an index).</p>"},{"location":"learning/cloud-data-warehouses/#references","title":"References","text":"<ul> <li>Blog post on BigQuery's strategy for data layout and storage</li> <li>BigQuery partitioning guide</li> <li>Snowflake documentation on clustering and micropartitions</li> <li>BigQuery optimization guide (most tips apply more generally to CDWs)</li> <li>Snowflake documentation on query performance profiling</li> </ul>"},{"location":"learning/dbt/","title":"dbt","text":"<p>Many CalData projects use dbt for transforming and modeling data within our cloud data warehouses. dbt has become extremely popular over the last several years, popularizing the practice and position of \"analytics engineering\". It has a number of features that makes it valuable for data stacks:</p> <ul> <li>It works well with version control</li> <li>It encourages modular, reusable SQL code</li> <li>It makes it easier to track data lineage as it flows through your data warehouse</li> <li>It has a large, active community with which you can share tips and techniques</li> </ul>"},{"location":"learning/dbt/#learning-dbt","title":"Learning dbt","text":"<p>dbt provides a series of free courses for learning how to use the project:</p> <ul> <li>dbt Fundamentals</li> <li>Jinja, Macros, and Packages</li> <li>Advanced Materializations</li> <li>Refactoring SQL for Modularity</li> </ul>"},{"location":"learning/git/","title":"git and GitHub","text":""},{"location":"learning/git/#what-are-git-and-github","title":"What are git and GitHub?","text":"<p>Git and GitHub occupy central positions in modern software development practices.</p> <p>Git is software that you install locally on your machine that enables source code management (SCM). Code is organized into folder-like structures called git repositories, which enables you to track the history of code, safely develop features in side branches, and collaborate with others in a distributed fashion. Git is a distributed version control system, which means that each developer has a local copy of the entire code repository. This makes it easy to work on code offline and to share changes with other developers.</p> <p></p> <p>GitHub is a web platform for hosting git repositories. It integrates tightly with local git development workflows, and includes additional features like a code review user interface, issue tracking, project boards, continuous integration/continuous delivery, and social networking. There are a number of web platforms which have similar features and goals as GitHub (including Bitbucket and GitLab), but GitHub is the most commonly used.</p> <p>Git and GitHub are essential tools for software development. They allow developers to track changes to code, collaborate, and share open or closed-source code.</p>"},{"location":"learning/git/#learning-git","title":"Learning git","text":"<p>There are many high-quality resources for learning git online. Here are a few:</p> <ul> <li>Atlassian has an excellent set of tutorials for learning git, including:<ul> <li>A conceptual overview for beginners</li> <li>How to set up a repository</li> <li>How to use git to collaborate with others</li> </ul> </li> <li>GitHub has a nice cheat-sheet for common git commands</li> <li>The official git documentation is not always the most user-friendly, but it has a depth of information that isn't available elsewhere</li> </ul>"},{"location":"learning/git/#learning-github","title":"Learning GitHub","text":"<p>In addition to the fundamentals of git, it's also helpful to know how to use the GitHub web platform for development. GitHub hosts an excellent set of interactive tutorials for learning to use its various features, including:</p> <ul> <li>An introduction to GitHub</li> <li>How to use Markdown for issues and READMEs</li> <li>How to review pull requests</li> <li>How to automate tasks and use CI/CD with GitHub actions</li> </ul>"},{"location":"learning/git/#git-and-github-at-caldata","title":"git and GitHub at CalData","text":"<p>On the CalData Data Services and Engineering team we make heavy use of git and GitHub for our projects, and have our own set of guidelines and best practices for code review.</p>"},{"location":"learning/git/#resources","title":"Resources","text":"<ul> <li>Install Git for Windows or GitHub for Desktop</li> <li>Bookmark GitHub\u2019s Git Cheat Sheet</li> </ul>"},{"location":"learning/git/#clone-the-project-repo-locally-for-macos-or-linux-based-cli","title":"Clone the project repo locally (for MacOS or Linux-based CLI)","text":"<ol> <li>Go to your command line</li> <li>Type <code>cd ~</code></li> <li>Type <code>git clone https://github.com/[repository_name].git</code></li> </ol>"},{"location":"learning/git/#clone-the-project-repo-locally-with-github-desktop","title":"Clone the project repo locally (with GitHub Desktop)","text":"<ol> <li>Follow these instructions: install GitHub Desktop</li> <li>Be sure to login with the GitHub username you are using for this project</li> <li>Be sure to select the cagov/[repository_name] repository to clone.</li> </ol>"},{"location":"learning/git/#pull-requests","title":"Pull Requests","text":"<p>A pull request (PR) is a way to propose changes to a code repository. It is a formal request to the repository owner(s) to merge your changes into their codebase, often the \u201cmain\u201d branch.</p> <p>PRs are typically created by cloning or forking the repository, making changes to the code, and then submitting a request to the repository owner to merge your changes. The repository owner can then review your changes, request changes \u2013 if necessary, and decide whether to merge them into their codebase.</p> <p>Here are some of the benefits of using PRs:</p> <ul> <li>Collaboration: PRs allow developers to collaborate on code and share ideas.</li> <li>Code review: PRs allow other developers to review your changes and provide feedback.</li> <li>Testing: PRs can be used to test changes before they are merged into the main codebase.</li> <li>Documentation: PRs can be used to document changes to the code.</li> <li>History: PRs provide a history of changes to the code.</li> </ul>"},{"location":"learning/git/#creating-or-switching-branches","title":"Creating or switching branches","text":"<ol> <li>Go to your Repository &gt; https://github.com/cagov/[repository_name]</li> <li>Click on the Branch Dropdown<ol> <li>On the main page of your repository, you'll see a dropdown menu on the top left, displaying the current branch (usually \u201cmain\u201d). Click on this dropdown.</li> </ol> </li> <li>Type or click on an Branch Name<ol> <li>In the dropdown that appears, there\u2019s a textbox at the top, type the name for the branch you want to switch to or create.</li> <li>When creating a new branch, choose a descriptive and relevant name. e.g. <code>project_staging_model</code></li> <li>If you\u2019re just switching branches you are done after this step!</li> </ol> </li> <li>Choose a base for the New Branch<ol> <li>Usually this is \u201cmain\u201d as you want to work off of the most up-to-date version of code, but you can also choose another branch to base your new one off of if you want to add to someone else\u2019s work in progress without making edits directly to their branch and causing a merge conflict! \ud83d\ude2c</li> </ol> </li> <li>Click \"Create branch <code>&lt;branch_name&gt;</code> from <code>&lt;base_branch_name&gt;</code>\" (usually your base branch will be \u201cmain\u201d)</li> </ol> <p>Now, you have successfully created a new branch in your GitHub repository and will be automatically switched to this branch. You can make changes on this new branch without affecting the main branch.</p> <p>If you're working with git locally, you can create and switch branches as well as check your current git branch using the following git commands:</p> <ul> <li>Create and switch to a branch: <code>git switch -c &lt;branch_name&gt;</code><ul> <li>This also works, but is the older way of doing it: <code>git checkout -b &lt;branch_name&gt;</code></li> </ul> </li> <li>Switch to an existing branch: <code>git switch &lt;branch_name&gt;</code><ul> <li>If this doesn\u2019t work it\u2019s likely that you have to pull down the remote branch you want to work with first using <code>git fetch</code></li> <li>Then you can run git switch <code>&lt;branch_name</code>&gt; again</li> </ul> </li> </ul> <p>Note</p> <pre><code>Replace `&lt;branch_name&gt;` with the actual name of your new branch e.g. `writing_project_docs`.\n</code></pre> <ul> <li>Check which branch you are on: <code>git branch --show-current</code></li> <li>Optional \u2013 persistently show your full path and branch name in command line:</li> </ul> <p>Mac OS</p> <ul> <li>Type <code>open ~/.bash_profile</code></li> <li> <p>In the file that opens, add the following at the bottom of the file:</p> <p><code>parse_git_branch() { git branch 2&gt; /dev/null | sed -e '/^[^*]/d' -e 's/* \\(.*\\)/ (\\1)/' } export PS1=\"\\u@\\h \\[\\033[32m\\]\\w -\\$(parse_git_branch)\\[\\033[00m\\] $ \"</code></p> </li> </ul> <p>Windows OS</p> <ul> <li>If you do not have git bash installed please download it here</li> <li>Showing branch names is a built-in feature of git bash. If branches do not show in your command prompt, follow the following steps.</li> <li>Download the official git-prompt script. Move it to somewhere accessible to git bash (e.g. <code>~/.git-prompt.sh</code>)</li> <li> <p>Add the following 2 lines to your ~/.bashrc file:</p> <p><code>source ~/.git-prompt.sh export PS1='\\[\\033[32m\\]\\u@\\h \\[\\033[35m\\]\\w\\[\\033[36m\\]$(__git_ps1 \" (%s)\")\\[\\033[0m\\]\\$ '</code></p> </li> </ul> <p>Follow the remaining steps regardless of OS</p> <ul> <li>Save the file and close it</li> <li>Open a new terminal window</li> <li>Type <code>cd ~/[folder_name]/</code></li> <li>You\u2019ll now see something like this:</li> </ul> <p></p> <p>Staging and committing changes</p> <ol> <li>Create or switch your branch<ol> <li>Make sure you\u2019re on the branch you want to commit changes to (you almost never want to commit directly to \u201cmain\u201d). If you just created a branch you are likely already on the branch you want to make changes to, if not switch to it.</li> <li>To create or switch branches, follow the steps above</li> </ol> </li> <li>Navigate to the file you want to modify and make the necessary changes<ol> <li>Whether you\u2019re editing a file in dbt Cloud, locally with a code editor like VS Code, or directly in GitHub (not recommended) you must click SAVE or use CTRL/COMMAND S. To save the changes to your file BEFORE you commit them</li> </ol> </li> <li>Stage your changes<ol> <li>We\u2019ll skip how to do this in GitHub because we don\u2019t recommend editing and committing changes directly in GitHub as this doesn\u2019t allow you to run linting tools that can help you catch errors BEFORE you push and change. We will have CI checks set up on the project repo that will help catch database, formatting, SQL errors etc.</li> <li>Locally this is done by:<ol> <li>Typing <code>git add &lt;file_name.file_extension&gt;</code></li> <li>You can use <code>git add .</code> to add ALL files you\u2019ve edited. This can be a dangerous operation because you may accidentally stage files you made changes to but did not want to be added to the project repo. For instance you could have made changes to a file that may contain sensitive information. Only use <code>git add .</code> if you are sure all files are safe to stage!</li> </ol> </li> <li>In dbt Cloud the git add process is handled under the hood so be sure that every file you edit is actually a file you want to later commit, if not you must revert changes to any files you do not want to commit.</li> </ol> </li> <li>Commit your changes<ol> <li>Again we\u2019ll skip how to do this in GitHub \u2013 we do not recommend it!</li> <li>Locally this is done with: <code>git commit -m \u201c&lt;a short message about the changes you made&gt;\u201d</code></li> <li>In dbt Cloud this is is done by:<ol> <li>Clicking the \u201ccommit and sync\u201d button</li> <li>Then type a short, yet descriptive message about the changes you made in the text box that appears and click \u201cCommit Changes\u201d</li> </ol> </li> </ol> </li> </ol> <p></p> <p>Pushing your changes</p> <ol> <li>Locally this is done with: <code>git push origin &lt;branch_name&gt;</code></li> <li>In dbt Cloud this is also done under the hood when you click \u201cCommit Changes\u201d</li> </ol> <p>Opening a PR</p> <p>Option 1: This works whether you commit changes locally or via dbt Cloud</p> <ol> <li>Go to the GitHub repository where you just pushed your changes</li> <li>At the top of the home page you\u2019ll see a message like the one below. It\u2019ll say \u201c<code>&lt;your_branch_name&gt;</code> had recent pushes X minutes ago\u201d with a green button that says \u201cCompare &amp; pull request\u201d. Click that button </li> <li>Next you\u2019ll be taken to a new screen like the one shown below. </li> <li>From here you\u2019ll:<ol> <li>Check that your branch is \u201cAble to merge\u201d (as seen in the upper center of the screen with a preceding green checkmark)<ol> <li>If you see \u201cCan\u2019t automatically merge.\u201d that means you have a merge conflict. We cover how to resolve merge conflicts below.</li> </ol> </li> <li>Add a more descriptive title and detailed description</li> <li>Add yourself as an Assignee</li> <li>Select a teammate as a Reviewer</li> <li>You\u2019ll have options to fill in other details like projects, we\u2019ll cover those later</li> </ol> </li> <li>Click the green button on the lower right that says \u201cCreate pull request\u201d</li> </ol> <p>Note</p> <pre><code>This option only works for an hour after you have pushed your changes. If you don\u2019t open a pull request within that 60 minute window this button will disappear. Fear not! There is a second way to open a pull request outlined below.\n</code></pre> <p>Option 2: This is the option to use if you cannot follow step 2 in Option 1.</p> <ol> <li>Go to the Pull requests page on GitHub by going directly to this link or go to the repo homepage and click on the \u201cPull requests\u201d tab near the top as pictured below </li> <li>Click the green \u201cNew pull request\u201d button in the upper right corner</li> <li>You\u2019ll be taken to a new window</li> <li>Click the button that says \u201ccompare: main\u201d</li> <li>A dropdown will open, from there you can either type or click the name of the branch you want to compare to the \u201cbase: main\u201d branch.</li> <li>After you select the branch follow steps 3 through 5 from Option 1 above</li> </ol> <p>Option 3: You have a third option to open a PR in dbt Cloud if you don\u2019t choose to follow the steps of one of the two options above.</p> <ol> <li>After you commit your changes you\u2019ll see a light green button on the upper left that says \u201cCreate a pull request on GitHub\u201d. This will only appear if you\u2019ve yet to open a PR. If you have already opened a PR and are simply committing more changes to it you will not see this option.</li> </ol>"},{"location":"learning/git/#reviewing-a-pr","title":"Reviewing a PR","text":"<p>The ODI CalData team put together documentation on reviewing a PR with the two core messages being, have empathy and get CI (continuous integration) to pass.</p> <ol> <li>Navigate to the PR<ol> <li>Go to the the \u201cPull Requests\u201d tab in project repository where the PR was opened and find the PR you want to review</li> <li>You can also find PRs where you are the requested reviewer by going directly to this link: github.com/pulls/review-requested</li> </ol> </li> <li>Review the changes<ol> <li>GitHub will take you to the home screen of the PR which starts on the \u201cConversation\u201d tab. This is where you can read any commits by the PR author, anyone involved in review, and any automated tools.</li> <li>The \u201cCommits\u201d tab is where you can check each save to the PR to understand the sequence of changes</li> <li>The \u201cChecks\u201d tab is where you can see the jobs run by GitHub actions (CI automations). You can see whether or not they pass or fail and the details of each.</li> <li>There will be a yellow pane across the top of this page like pictured below. </li> </ol> </li> </ol> <p>Clicking the green \u201cAdd your review\u201d button will take you to the \u201cFiles changed\u201d tab where you can begin your review.</p> <p>In the files changed tab you can leave a comment on any line of code by clicking the blue plus sign that appears when you hover. You can leave a single comment that is not part of a review or leave comments as part of your review. You can suggest changes here too which we\u2019ll cover in the next section.</p> <p>After you\u2019re done with your review, if you scroll back to the top there will be a green button on the upper right that says \u201cFinish your review\u201d. Click that and decide if you just want to do one of the following: 1) Comment, 2) Approve or 3) Request changes. Then click the green button on the lower right that says \u201cSubmit review\u201d.</p>"},{"location":"learning/git/#suggesting-changes-to-a-pr","title":"Suggesting changes to a PR","text":"<p>When you\u2019re reviewing a PR instead of just commenting on a line of code you may want to suggest changes directly to the code. You can do this by clicking the blue plus sign button next to the line of code you want to suggest changes to.</p> <p>In the window that opens click the button that has a + and - sign as pictured below.  In this example, \u201ctest\u201d is misspelled so the PR review is adding a suggestion and fixing the code with the correct spelling. If the PR author agrees they can seamlessly accept this suggestion and integrate it into their code.</p> <p>After adding your suggestion and additional comments if applicable, click the green \u201cStart a review\u201d button.</p>"},{"location":"learning/git/#resolving-a-merge-conflict","title":"Resolving a merge conflict","text":"<p>Merge conflicts occur in git when the software is unable to automatically merge changes from two different branches. This happens when changes are made to the same part of a file in both the source and target branches (these are often interchanged as feature and destination branches, respectively) and git is unable to determine which of the changes should be kept.</p> <p>Below we\u2019ll step through a more detailed explanation of how a merge conflict happens and how to resolve it:</p> <ol> <li>Parallel changes occur<ol> <li>Let\u2019s say you\u2019re working on a branch called feature_branch where you\u2019re working on a file called docs.md. Meanwhile, a teammate makes changes to that same file in another branch feature_branch_2 that gets merged to the main branch before yours does.</li> </ol> </li> <li>Git attempts a merge<ol> <li>When you try to merge feature_branch into the main branch, git will attempt to merge the changes. If the changes are in different parts of the same file, git can usually merge them seamlessly.</li> </ol> </li> <li>Conflict arises<ol> <li>If changes are made to the same part of the same file in both branches, git will detect a conflict. It is unable to reconcile the differences in files as it doesn\u2019t know which changes to keep and which to discard.</li> </ol> </li> <li>Conflict markers<ol> <li>If attempting to merge from your command line you\u2019ll see the following </li> <li>Git will also mark the conflicting sections of the file with special markers. The conflicting changes from both branches are placed between these markers. Like you see below </li> <li>If you navigate to GitHub to create a pull request you\u2019ll see the following </li> </ol> </li> <li>Resolution<ol> <li>Manually<ol> <li>To resolve the conflict you need to manually edit the file to choose what changes to keep. Remove the conflict markers and any code that is not needed.</li> <li>After you resolve the conflict you\u2019ll need to stage the file again with <code>git add &lt;file_name.file_extension&gt;</code></li> <li>You\u2019ll need to commit the changes again with <code>git commit - m \u201c&lt;a short message about the changes you made&gt;\u201d</code></li> <li>Type <code>git merge --continue</code></li> <li>Finally, type <code>git push origin main</code></li> </ol> </li> <li>In VS Code<ol> <li>You\u2019ll see the following along with a UI to actually help you decide which changes to keep </li> <li>In the lower right corner of your screen you\u2019ll see a blue button that says \u201cResolve in Merge Editor\u201d. Click this button.</li> <li>Next you\u2019ll be taken to a screen like you see below. On this screen you\u2019ll have options on either side of your Incoming and Current changes to select the following options: Accept Incoming | Accept Combination | Ignore</li> <li>Select the appropriate option, this may require discussion with your team.</li> <li>After you decide which changes to keep, click the blue \u201cComplete Merge\u201d button in the lower right corner of  your screen </li> </ol> </li> </ol> </li> </ol> <p>To avoid or reduce the occurrence of merge conflicts, it\u2019s a good practice to regularly pull changes from the main branch into your feature branch. Open communication with your team about changes will also help prevent conflicts.</p>"},{"location":"learning/git/#merging-a-pr","title":"Merging a PR","text":"<p>As a reviewer you have the responsibility to merge a PR after you approve it. PRs should not be merged if any of the following are true: - All CI checks have not passed (2 out of 3 passing isn\u2019t good enough) - You haven\u2019t approved the PR - Requested changes have not be addressed or planned for in another PR - The \u2018Merge pull request\u201d button is green</p> <p>If no changes are needed or if requested changes are made and all CI checks have passed, you can merge the PR by clicking the \u201cMerge pull request\u2019 button on the \u201cConversation\u201d tab of the PR.</p> <p>After you click this,  GitHub will prompt you to choose which email to associate with the commit and then to click the \u201cConfirm merge\u201d button.</p> <p>At this point, GitHub will finalize the merge process and these changes will now be incorporated into the main branch. You may delete the feature branch that was used to submit the PR.</p>"},{"location":"learning/git/#github-issues","title":"GitHub Issues","text":"<p>Issues on GitHub are how we document and track our work. Writing a good issue is important because it provides clear information on the what, why, and who relating to a body of work which can enable efficient collaboration. Well documented issues allow for quicker understanding and problem resolution. They serve as a form of documentation capturing decisions and discussion which is valuable to reference for current and future team members. Clear issues also help with task prioritization and milestone tracking across our work.</p>"},{"location":"learning/git/#creating-an-issue","title":"Creating an issue","text":"<ol> <li>Go to the \u201cIssues\u201d tab of the project repository</li> <li>Find and click on the green \u201cNew issue\u201d button in the upper right corner</li> <li>At a minimum your issue should have 5 main elements and 5 more sub-elements.<ol> <li>A clear, concise, and descriptive title</li> <li>A detailed description of the issue 3.An assignee, usually yourself, but you may assign someone else more appropriate</li> <li>A project it corresponds to</li> <li>After the issue is created click on the dropdown next to your project in the \u201cProjects\u201d section on the right side of your issue. Add the following details:<ol> <li>Status (Backlog, To do, In progress, Needs Review, Blocked, etc.)</li> <li>Priority (e.g. Urgent, High, Medium, Low)</li> <li>Size (X-Large, Large, Medium, Small, Tiny)</li> <li>Sprint (either current or future)</li> <li>Project (e.g. DIF - Caltrans)</li> </ol> </li> </ol> </li> <li>A relevant milestone that this issue fits into</li> </ol> <p>Here is [an example GitHub issue] (https://github.com/cagov/caldata-mdsa-caltrans-pems/issues/31)that has all 10 elements and sub-elements listed above.</p>"},{"location":"learning/git/#writing-markdown","title":"Writing Markdown","text":"<p>Writing markdown is important to learn when creating project documentation in markdown files (.md) and for writing GitHub issues. GitHub has handy documentation on basic Markdown syntax which is a great starting point.</p>"},{"location":"learning/git/#github-magic","title":"GitHub Magic \u2728","text":"<ol> <li>Linking PRs to Issues and Issues to PRs<ol> <li>You can do this by going to the \u201cDevelopment\u201d section of your issue on the right hand side underneath \u201cMilestone\u201d. Click on the cog wheel then select the relevant repository the PR lives in. From there type in the number (e.g. 266) or title of the PR you want to link or select it from the dropdown that appears.</li> <li>You can also do this by using a keyword like \u201ccloses, fixes, or resolves\u201d followed by # and the issue number, e.g. Fixes #143</li> <li>When you use the \u201ccloses\u201d keyword, this will close the corresponding issue when the PR is merged</li> <li>For a full list of keywords you can use to link a PR to an issue checkout GitHub\u2019s documentation complete with examples</li> </ol> </li> <li>Tagging a teammate<ol> <li>This is done by typing @ and selecting or typing your teammate\u2019s username in the pop-up window that appears</li> <li>You can tag teammates in your commit message, issue or PR comments, and issue descriptions</li> </ol> </li> </ol>"},{"location":"learning/glossary/","title":"Modern data stack glossary","text":"<p>This glossary is a reference for commonly used acronyms, terms, and tools associated with the modern data stack and data and analytics engineering practices.</p>"},{"location":"learning/glossary/#acronyms","title":"Acronyms","text":"<ul> <li>MDS - Modern Data Stack</li> <li>ETL - Extract, Transform, Load</li> <li>ELT - Extract, Load, Transform</li> <li>CI - Continuous integration</li> <li>CD - Continuous delivery/deployment</li> <li>GCS - Google Cloud Storage</li> <li>AWS - Amazon Web Services</li> <li>SaaS - Software as a Service</li> </ul>"},{"location":"learning/glossary/#definitions","title":"Definitions","text":"<ol> <li> <p>Modern data stack - a cloud-first suite of software tools that enable data teams to connect to, process, store, transform, and visualize data.</p> </li> <li> <p>ETL vs ELT \u2013 ETL (Extract, Transform and Load) and ELT (Extract, Load and Transform) are data integration methods that determine whether data is preprocessed before landing in storage or transformed after being stored.</p> <p>Both methods have the same three operations:</p> <ul> <li>Extraction: Pulling data from its original source system (e.g. connecting to data from a SaaS platform like Google Analytics)</li> <li>Transformation: Changing the data\u2019s structure so it can be integrated into the target data system. (e.g. changing geospatial data from a JSON structure to a parquet format)</li> <li>Loading: Dumping data into a storage system (e.g. AWS S3 bucket or GCS)</li> </ul> <p>Advantages of ELT over ETL:</p> <ul> <li>More flexibility, as ETL is traditionally intended for relational, structured data. Cloud-based data warehouses enable ELT for structured and unstructured data</li> <li>Greater accessibility, as ETL is generally supported, maintained, and governed by organizations\u2019 IT departments. ELT allows for easier access and use by employees</li> <li>Scalability, as ETL can be prohibitively resource-intensive for some businesses. ELT solutions are generally cloud-based SaaS, available to a broader range of businesses</li> <li>Faster load times, as ETL typically takes longer as it uses a staging area and system. With ELT, there is only one load to the destination system</li> <li>Faster transformation times, as ETL is typically slower and dependent on the size of the data set(s). ELT transformation is not dependent on data size</li> <li>Less time required for data maintenance, as data may need to be re-sourced and re-loaded if the transformation is found to be inadequate for the data\u2019s intended purposes. With ELT, the original data is intact and already loaded from disk</li> </ul> <p>Sources: Fivetran, Snowflake</p> </li> <li> <p>Columnar Database vs Relational Database - a columnar database stores data by columns making it suitable for analytical query processing whereas a relational database stores data by rows making it optimized for transactional applications</p> <p>Advantages of Columnar over Relational databases:</p> <ul> <li>Reduces amount of data needed to be loaded</li> <li>Improves query performance by returning relevant data faster (instead of going row by row, multiple fields can be skipped)</li> </ul> </li> <li> <p>Cloud Data Warehouse - a database stored as a managed service in a public cloud optimized for scalable analytics.</p> <p>Tools like Excel, Tableau, or PowerBI are limited in how much data can be brought into the dashboard. Cloud data warehouses, however, can handle petabyte scale data without too much fuss. Now, PowerBI or Tableau can also pass off data processing to a cloud data warehouse, but then major data processing jobs get hidden in a dashboard panel, which can produce unexpected spends, poor code reusability, and brittle dashboards.</p> </li> <li> <p>Analytics engineering - applies software engineering practices to analytical workflows like version control and continuous integration/development. Analytics engineers are often thought of as a hybrid between data engineers and data analysts. Most analytics engineers spend their time transforming, modeling, testing, deploying, and documenting data. Data modeling \u2013 applying business logic to data to represent commonly known truths across an organization (for example, what data defines an order) \u2013 is the core of their workload and it enables analysts and other data consumers to answer their own questions.</p> <p>Originally coined two years ago, by Michael Kaminsky, the term came from the ground-up, when data people experienced a shift in their job: they went from handling data engineer/scientist/analyst\u2019s tasks to spending most of their time fixing, cleaning, and transforming data. And so, they (mainly members of the dbt community) created a terminology to describe this middle seat role: the Analytics Engineer.</p> <p>dbt comes in as a SQL-first transformation layer built for modern data warehousing and ingestion tools that centralizes data models, tests, and documentation.</p> <p>Sources: Castor, dbt</p> </li> <li> <p>Agile development - is an iterative approach to software development (and project management) that helps teams ship code faster and with fewer bugs.</p> <ul> <li>Sprints - a time-boxed period (usually 2 weeks) when a team works to complete a set amount of work. Some sprints have themes like if a new tool was procured an entire sprint may be dedicated to setup and onboarding.</li> </ul> <p>Additional reading: Atlassian: What is Agile?, Adobe: Project Sprints</p> </li> <li> <p>Version control - enables teams to collaborate and streamline code development to resolve conflicts and create a centralized location for code.</p> <p>Source: Gitlab: What is Version Control</p> </li> <li> <p>CI/CD - Continuous integration and continuous delivery/deployment are automated processes to deploy code (e.g., whenever you merge to main).</p> <ul> <li>Continuous integration (CI) automatically builds, tests, and integrates code changes within a shared repository</li> <li>Continuous delivery (CD) automatically delivers code changes to production environments for human approval</li> <li>~OR ~ Continuous deployment (CD) automatically delivers and deploys code changes directly, circumventing human approval</li> </ul> <p>Source: Github: CI/CD explained</p> <p>Some advantages of using CI/CD are:</p> <ul> <li>Check code quality with linters in pull requests</li> <li>Run data/code tests in pull requests</li> <li>Deploy documentation upon merging to main</li> <li>Deploy infrastructure upon merging to main</li> </ul> </li> </ol>"},{"location":"learning/glossary/#tools","title":"Tools","text":"<ul> <li>Fivetran - a data loading tool that connects source data to your data warehouse.</li> <li>dbt - a data transformation/modeling tool that operates as a layer on top of your data warehouse.</li> <li>Airflow - an open-source data orchestration tool for developing, scheduling, and monitoring batch-oriented workflows.</li> <li>Snowflake - a cloud data warehouse that can automatically scale up/down compute resources to load, integrate, and analyze data.</li> <li>GitHub - a cloud-based Git repository for version control.</li> <li>AWS S3 - cloud-based object storage.</li> </ul>"},{"location":"learning/naming-conventions/","title":"Naming conventions","text":"<p>This page documents the Data Services and Engineering (DSE) team's naming conventions for cloud resources.</p>"},{"location":"learning/naming-conventions/#general-approach","title":"General approach","text":"<p>Our approach is adapted from this blog post. The goals of establishing a naming convention are: 1. Prevent name collisions between similar resources (especially in cases where names are required to be unique). 1. Allow developers to identify at a glance what a particular resource is and who owns it. 1. Structured naming allows for easier sorting and filtering of resources.</p> <p>The overall name template is:</p> <p><pre><code>{owner}-{project}-{env}-[{region}]-[{description}]-[{suffix}]\n</code></pre> Where <code>{...}</code> indicates a component in the name, and <code>[{...}]</code> indicates that it is optional or conditionally required.</p> Component Description Required Constraints owner Owner of the resource \u2714 len 3-6 project Project name \u2714 len 4-10, a-z0-9 env Environment type, e.g. <code>dev</code>, <code>prd</code>, <code>stg</code> \u2714 len 3, a-z, enum region Region (if applicable) \u2717 enum description Additional description (if needed) \u2717 len 1-20, a-z0-9 suffix Random suffix (only use if there are multiple identical resources) \u2717 len 3, a-z0-9 <p>Owner: This is a required field. For most of our projects, it will be <code>dse</code> (for Data Services and Engineering), though it could be other things for projects that we will be handing off to clients upon completion.</p> <p>Project: A short project name. This is a required field. For general DSE infrastructure, use <code>infra</code>.</p> <p>Environment: The deployment environment. This is a required field. Generally <code>prd</code> (production), <code>stg</code> (staging), or <code>dev</code> (development).</p> <p>Region: If the resource exists in a particular region (e.g. <code>us-west-1</code>), this should be included.</p> <p>Description: There may be multiple resources that are identical with respect to the above parameters, but have a different purpose. In that case, append a <code>description</code> to the name to describe that purpose. For instance, we might have multiple subnets, some of which are <code>public</code> and some of which are <code>private</code>. Or we could have multiple buckets for storing different kinds data within the same project.</p> <p>Suffix: If there all of the above are identical (including <code>description</code>), include a random suffix. This can be accomplished with the terraform <code>random_id</code> resource.</p>"},{"location":"learning/naming-conventions/#examples","title":"Examples","text":"<p>GCP Project: We have a GCP project for managing web/product analytics collaborations with the CalInnovate side of ODI. GCP projects to not exist in a region, so a production project could be called <code>dse-product-analytics-prd</code>.</p> <p>MWAA Environment: An Apache Airflow environment in AWS does exist in a region, and supports general DSE infrastructure. So a development deployment of the environment could be <code>dse-infra-dev-us-west-2</code>.</p> <p>Scratch bucket: We might have several S3 buckets supporting different aspects of a project. For instance, one bucket could be used for scratch work, and another could be used for source data. The scratch bucket could then be named <code>dse-infra-dev-us-west-1-scratch</code>.</p>"},{"location":"learning/naming-conventions/#resource-tagging","title":"Resource tagging","text":"<p>Cloud resources can be tagged with user-specified key-value pairs which allow for resource and cost tracking within a given cloud account.</p> <p>Our tagging convention is that information which is available in the resource name should also be available in tags as a specific key-value pair:</p> Key Value Required Owner <code>{owner}</code> \u2714 Project <code>{project}</code> \u2714 Environment <code>{env}</code> \u2714 Description <code>{description}</code> \u2717 <p>Note that the <code>{region}</code> and <code>{suffix}</code> components are not included. This is because the region information is typically available elsewhere in the API/Console, and the suffix information is not semantically meaningful.</p>"},{"location":"learning/naming-conventions/#specific-considerations","title":"Specific considerations","text":"<p>Not all resources can follow the above convention exactly. For instance, some resource names may not allow hyphens, or may have length limits. In those cases, we should try to adhere to the conventions as closely as possible (e.g., by substituting underscores for hyphens) and document the exception here.</p>"},{"location":"learning/naming-conventions/#cloud-data-warehouse-schemas","title":"Cloud data warehouse schemas","text":"<p>Data warehouse schemas (or datasets in BigQuery) are often user/analyst-facing, and have different considerations. They usually cannot have hyphens in them, so words should be separated with underscores \"<code>_</code>\". Furthermore, analysts needn't need to know details of regions or deployments, so <code>region</code> and <code>env</code> are dropped, and the naming convention becomes:</p> <pre><code>{owner}_{project}_[{description}]\n</code></pre> <p>If a project is owned by the Data Services and Engineering team, the <code>owner</code> component may be omitted, and the schema name is simply <pre><code>{project}_[{description}]\n</code></pre> Note that Snowflake normalizes all object names to upper case. This is opposite to how PostgreSQL normalizes object names (sigh). Most of the time this doesn't matter, but occasionally requires thought if you have a mixed-case object name. If you are naming new database tables or schemas, mixed-case identifiers should be avoided.</p>"},{"location":"learning/naming-conventions/#dbt","title":"dbt","text":"<p>Our dbt naming conventions are described here.</p>"},{"location":"learning/naming-conventions/#fivetran","title":"Fivetran","text":"<p>The names of tables loaded by Fivetran are typically set by either Fivetran or the names of the tables in the source systems. As such, we don't have much control over them, and they won't adhere to any particular naming conventions.</p> <p>Fivetran connectors names cannot contain hyphens, and should follow this pattern:</p> <p><pre><code>fivetran_{owner}_{project}_{connector_type}_[{description}]\n</code></pre> The schemas into which a fivetran connector is writing should be named the same as the connector (which is why the connector name has some seemingly redundant information).</p> <p>If a project is owned by the Data Services and Engineering team, the <code>owner</code> component may be omitted, and the schema name is simply <pre><code>fivetran_{project}_{connector_type}_[{description}]\n</code></pre></p>"},{"location":"learning/naming-conventions/#external-references","title":"External References","text":"<ul> <li>https://docs.getdbt.com/blog/stakeholder-friendly-model-names</li> <li>https://docs.getdbt.com/blog/on-the-importance-of-naming</li> </ul>"},{"location":"learning/security/","title":"Security guidelines","text":"<p>This document describes security conventions for CalData's Data Services and Engineering team, especially as it relates to cloud and SaaS services.</p>"},{"location":"learning/security/#cloud-security-and-iam","title":"Cloud security and IAM","text":"<p>The major public clouds (AWS, GCP, Azure) all have a service for Identity and Access Management (IAM). This allows us to manage which users or services are able to perform actions on which resources. In general, IAM is described by:</p> <ul> <li>Users (or principals) - some person or workflow which uses IAM to access cloud resources. Users can be assigned to groups.</li> <li>Permissions - an ability to perform some action on a resource or collection of resources.</li> <li>Groups - Rather than assigning permissions directly to users, it is considered good practice to instead create user groups with appropriate permissions, then add users to the group. This makes it easier to add and remove users while maintaining separate user personas.</li> <li>Policies - a group of related permissions for performing a job, which can be assigned to a role or user.</li> <li>Role - a group of policies for performing a workflow. Roles are similar to users, but do not have a user identity associated with them. Instead, they can be assumed by users or services to perform the relevant workflow.</li> </ul> <p>Most of the work of IAM is managing users, permissions, groups, policies, and roles to perform tasks in a secure way.</p>"},{"location":"learning/security/#principle-of-least-privilege","title":"Principle of least-privilege","text":"<p>In general, users and roles should be assigned permissions according to the Principle of Least Privilege, which states that they should have sufficient privileges to perform legitimate work, and no more. This reduces security risks should a particular user or role become compromised.</p> <p>Both AWS and GCP have functionality for analyzing the usage history of principals, and can flag permissions that they have but are not using. This can be a nice way to reduce the risk surface area of a project.</p>"},{"location":"learning/security/#service-accounts","title":"Service accounts","text":"<p>Service accounts are special IAM principals which act like users, and are intended to perform actions to support a particular workflow. For instance, a system might have a \"deploy\" service account in CD which is responsible for pushing code changes to production on merge.</p> <p>Some good practices around the use of service accounts (largely drawn from here):</p> <ul> <li>Service accounts often have greater permissions than human users,   so user permissions to impersonate these accounts should be monitored!</li> <li>Don't use service accounts during development (unless testing the service account permissions).   Instead, use your own credentials in a safe development environment.</li> <li>Create single-purpose service accounts, tied to a particular application or process.   Different applications have different security needs,   and being able to edit or decommission accounts separately from each other is a good idea.</li> <li>Regularly rotate access keys for long-term service accounts.</li> </ul>"},{"location":"learning/security/#production-and-development-environments","title":"Production and development environments","text":"<p>Production environments should be treated with greater care than development ones. In the testing and developing of a service, roles and policies are often crafted which do not follow the principle of least privilege (i.e., they have too many permissions).</p> <p>When productionizing a service or application, make sure to review the relevant roles and service accounts to ensure they only have the necessary policies, and that unauthorized users don't have permission to assume those roles.</p>"},{"location":"learning/security/#gcp-practices","title":"GCP practices","text":"<p>GCP's IAM documentation is a good read, and goes into much greater detail than this document on how to craft and maintain IAM roles.</p> <p>Default GCP service accounts often have more permissions than are strictly needed for their intended operation. For example, they might have read/write access to all GCS buckets in a project, when their application only requires access to one.</p>"},{"location":"learning/security/#aws-practices","title":"AWS practices","text":"<p>AWS has a nice user guide for how to work with IAM, including some best-practices.</p>"},{"location":"learning/security/#third-party-saas-integrations","title":"Third-party SaaS integrations","text":"<p>Often a third-party software-as-a-service (SaaS) provider will require service accounts to access resources within a cloud account. For example, <code>dbt</code> requires fairly expansive permissions within your cloud data warehouse to create, transform, and drop data.</p> <p>Specific IAM roles needed for a SaaS product are usually documented in their setup guides. These should be periodically reviewed by CalData and ODI IT-Ops staff to ensure they are still required.</p>"},{"location":"learning/security/#fivetran-practices","title":"Fivetran practices","text":"<p>Fivetran's security docs which link to a deeper dive white paper are a good place to go to understand their standards and policies for connecting, replicating, and loading data from all of our data sources.</p> <p>Within the Users &amp; Permissions section of our Fivetran account there are three sub-sections for: Users, Roles, and Teams.</p> <p>Fivetran also provides detailed docs on Role-Based Access Controls (RBAC) which covers the relationship among users, roles, and teams when defining RBAC policies.</p> <p>This diagram from their docs gives an at-a-glance view of how RBAC could be configured across multiple teams, destinations, and connectors. Since we aren't a massive team, we may not have as much delegation, but this gives you a sense of what's possible.</p> <p> See diagram in context of docs.</p> <p>Note: With the recent creation of organizations in Fivetran, client project security policies get simplified as we can isolate them from our other projects by creating a separate account. This is also better from a handoff perspective. The Roles page defines all default role types and how many users are associated with each type. There is also the ability to create custom roles via the + Add Role button.</p> <p>Currently, we have two teams: - IT - Data Services and Engineering (DSE)</p> <p>Our IT team's role is Account Billing and User Access. This is a custom role that provides billing and user management access with view-only access for the remaining account-level features; it provides no destination or connector access.</p> <p>The DSE team both manages CalData projects and onboards clients into Fivetran, and so its members have Account Administrator roles.</p>"},{"location":"learning/security/#iam-through-infrastructure-as-code","title":"IAM through infrastructure-as-code","text":"<p>(TODO)</p>"},{"location":"learning/security/#snowflake","title":"Snowflake","text":"<p>Security policies for Snowflake can be found here.</p>"},{"location":"learning/security/#security-review-standard-practices","title":"Security review standard practices","text":"<p>(TODO, possibly pulling from Agile Application Security)</p>"},{"location":"setup/dbt-setup/","title":"dbt project setup","text":"<p>To set up a new project on dbt Cloud follow these steps:</p> <ol> <li>Give your new project a name.</li> <li>Click Advanced settings and in the Project subdirectory field, enter \"transform\"</li> <li>Select a data warehouse connection. (e.g. Snowflake, BigQuery, Redshift)</li> <li> <p>For the Development credentials section you'll want to choose between Snowflake OAuth or Key pair. In general, Snowflake OAuth is preferred for human users (which is what the development environment is for). It is also an enterprise dbt Cloud feature, so if working with a standard account, you'll need to use key pair.</p> </li> <li> <p>For Snowflake OAuth:</p> <ol> <li>Follow dbt's instructions for set up here</li> </ol> </li> <li> <p>For Key pair:</p> <ol> <li>Under Auth method select Key pair</li> <li>Enter your data warehouse username</li> <li>Enter the private key and private key passphrase</li> <li>For more guidance, read dbt's docs on connecting to Snowflake via key pair</li> </ol> </li> <li> <p>Finally click the Test Connection button.</p> </li> <li>Connect the appropriate repository. Read dbt's docs on connecting to GitHub or dbt's docs on connecting to Azure DevOps and Microsoft's docs on creating branch policies in DevOps. To integrate dbtCloud with Azure DevOps, the service user (legacy) option must be used.  Complete the steps found in the documentation.</li> </ol> <p>Once you're through the first five steps you can return to the dbt homepage and click the Settings button in the upper right corner. From there you can follow the steps to configure three environments for Continuous integration - CI, development, and production. Read dbt's docs on CI in dbt Cloud. Read dbt's docs on creating production (deployment) environments and dbt's docs on creating and scheduling deploy jobs.</p> <p>You'll also want to configure notifications for job failures.</p> <p>Pictured below is an example of environment variables you can set for each environment. For more guidance, read dbt's docs on environment variables.</p> <p></p>"},{"location":"setup/fivetran-setup/","title":"Fivetran project setup","text":"<p>To set up a new project in Fivetran follow these steps:</p> <ol> <li> <p>First, ensure you have met the following pre-requisites:</p> <ul> <li>You have set up a Snowflake Account for the project (follow all instructions from here)</li> <li>Ensure that your Snowflake project has a <code>LOADER_PRD</code> role with privileges to write data to the <code>RAW_PRD</code> database</li> <li>You have created a Snowflake User called <code>FIVETRAN_SVC_USER_PRD</code> and ensured this user has the <code>LOADER_PRD</code> role</li> <li>You have set up an auth key pair for this user and saved it to the ODI OnePass account</li> </ul> </li> <li> <p>In Fivetran, navigate to Organization -&gt; Accounts</p> </li> <li>Click Add Acount</li> <li>Choose an Account Name, select Enterprise for Account Tier and No restrictions for Required Authentication Type</li> <li>Next, navigate to Destinations</li> <li>Search for Snowflake and click Select</li> <li>To set up the Snowflake connector:<ol> <li>Name the destination <code>RAW_PRD</code></li> <li>Add the Snowflake URL for your project as the Host</li> <li>Add <code>FIVETRAN_SVC_USER_PRD</code> as the User</li> <li>Add <code>RAW_PRD</code> as the Database</li> <li>For Auth select KEY_PAIR and enter the key pair details for the <code>FIVETRAN_SVC_USER_PRD</code> user</li> <li>Add <code>LOADER_PRD</code> as the Role</li> <li>Optional: Most of the time, the cloud provider and region don't matter, but if a client is operating in a particular cloud/region and wants to minimize data transfer, it makes sense to select the client's Cloud service provider, Cloud region, and Default Time Zone</li> <li>Click the Save &amp; Test button</li> </ol> </li> </ol> <p>Once you are through with these steps, you can proceed to creating and assigning permissions to Users in the Fivetran account.</p>"},{"location":"setup/project-teardown/","title":"Tearing down a project","text":"<p>Upon completion of a project (or if you just went through project setup for testing purposes) there are a few steps needed to tear down the infrastructure.</p> <ol> <li>If the GitHub repository is to be handed off a client, transfer ownership of it to them.     Otherwise, delete or archive the GitHub repository.     If archiving, delete the GitHub actions secrets.</li> <li>Open a Help Desk ticket with IT-Ops to remove Sentinel logging for the Snowflake account.</li> <li>If the Snowflake account is to be handed off to a client, transfer ownership of it to them.     Otherwise, drop the account.</li> </ol>"},{"location":"setup/repo-setup/","title":"git/Github setup","text":""},{"location":"setup/repo-setup/#create-project-git-repository","title":"Create project git repository","text":"<p>Create a new git repository from the CalData Infrastructure Template following the instructions here.</p> <p>Once you have created the repository, push it to a remote repository in GitHub. There are some GitHub actions that will fail because the repository is not yet configured to work with the new Snowflake account.</p>"},{"location":"setup/repo-setup/#set-up-ci-in-github","title":"Set up CI in GitHub","text":"<p>The projects generated from our infrastructure template need read access to the Snowflake account in order to do two things from GitHub actions:</p> <ol> <li>Verify that dbt models in branches compile and pass linter checks</li> <li>Generate dbt docs upon merge to <code>main</code>.</li> </ol> <p>The terraform configurations deployed above create two service accounts for GitHub actions, a production one for docs and a dev one for CI checks.</p>"},{"location":"setup/repo-setup/#add-key-pairs-to-the-github-service-accounts","title":"Add key pairs to the GitHub service accounts","text":"<p>Set up key pairs for the two GitHub actions service accounts (<code>GITHUB_ACTIONS_SVC_USER_DEV</code> and <code>GITHUB_ACTIONS_SVC_USER_PRD</code>). This follows a similar procedure to what you did for your personal key pair, though the project template currently does not assume an encrypted key pair. This bash script is a helpful shortcut for generating the key pair: <pre><code>bash generate_key.sh &lt;key-name&gt;\n</code></pre></p> <p>Once you have created and set the key pairs, add them to the DSE 1Password shared vault. Make sure to provide enough information to disambiguate the key pair from others stored in the vault, including:</p> <ul> <li>The account locator (legacy account identifier)</li> <li>The organization name</li> <li>The account name (distinct from the account locator)</li> <li>Note : The preferred account identifier is to use name of the account prefixed by its organization (e.g. myorg-account123)</li> <li>The service account name</li> <li>The public key</li> <li>The private key</li> </ul>"},{"location":"setup/repo-setup/#set-up-github-actions-secrets","title":"Set up GitHub actions secrets","text":"<p>You need to configure secrets in GitHub actions in order for the service accounts to be able to connect to your Snowflake account. From the repository page, go to \"Settings\", then to \"Secrets and variables\", then to \"Actions\".</p> <p>Add the following repository secrets:</p> Variable Value <code>SNOWFLAKE_ACCOUNT</code> new account locator <code>SNOWFLAKE_USER_DEV</code> <code>GITHUB_ACTIONS_SVC_USER_DEV</code> <code>SNOWFLAKE_USER_PRD</code> <code>GITHUB_ACTIONS_SVC_USER_PRD</code> <code>SNOWFLAKE_PRIVATE_KEY_DEV</code> dev service account private key <code>SNOWFLAKE_PRIVATE_KEY_PRD</code> prd service account private key"},{"location":"setup/repo-setup/#enable-github-pages-for-the-repository","title":"Enable GitHub pages for the repository","text":"<p>The repository must have GitHub pages enabled in order for it to deploy and be viewable.</p> <ol> <li>From the repository page, go to \"Settings\", then to \"Pages\".</li> <li>Under \"GitHub Pages visibility\" select \"Private\" (unless the project is public!).</li> <li>Under \"Build and deployment\" select \"Deploy from a branch\" and choose \"gh-pages\" as your branch.</li> </ol>"},{"location":"setup/sentinel-setup/","title":"Set up Sentinel logging","text":"<p>ODI IT requires that systems log to our Microsoft Sentinel instance for compliance with security monitoring policies. The terraform configuration deployed above creates a service account for Sentinel which needs to be integrated.</p> <ol> <li>Create a password for the Sentinel service account.     In other contexts we prefer key pairs for service accounts, but the Sentinel     integration requires password authentication. In a Snowflake worksheet run:     <pre><code>use role securityadmin;\nalter user sentinel_svc_user_prd set password = '&lt;new-password&gt;'\n</code></pre></li> <li> <p>Store the Sentinel service account authentication information in our shared     1Password vault.     Make sure to provide enough information to disambiguate it from others stored in the vault,     including:</p> <ul> <li>The account locator (legacy account identifier)</li> <li>The organization name</li> <li>The account name (distinct from the account locator)</li> <li>Note : The preferred account identifier is to use name of the account prefixed by its organization (e.g. myorg-account123)</li> <li>The service account name</li> <li>The public key</li> <li>The private key</li> </ul> </li> <li> <p>Create an IT Help Desk ticket to add the new account to our Sentinel instance.     Share the 1Password item with the IT-Ops staff member who is implementing the ticket.     If you've included all of the above information in the vault item,     it should be all they need.</p> </li> <li>Within fifteen minutes or so of implementation it should be clear whether the integration is working.     IT-Ops should be able to see logs ingesting, and Snowflake account admins should see queries     from the Sentinel service user.</li> </ol>"},{"location":"setup/snowflake-service-accounts/","title":"Adding service accounts","text":""},{"location":"setup/snowflake-service-accounts/#create-service-accounts-using-terraform","title":"Create service accounts using Terraform","text":"<p>Service accounts aren't associated with a human user. Instead, they are created by an account administrator for the purposes of allowing another service to perform some action.</p> <p>We currently use service accounts for:</p> <ul> <li>Fivetran loading raw data</li> <li>Airflow loading raw data</li> <li>dbt Cloud for transforming data</li> <li>GitHub actions generating docs</li> </ul> <p>These service accounts are created using Terraform and assigned roles according to the principle of least-privilege. They use key pair authentication, which is more secure than password-based authentication as no sensitive data are exchanged. Private keys for service accounts should be stored in CalData's 1Password vault.</p> <p>The following are steps for creating a new service account with key pair authentication:</p> <ol> <li>Create a new key pair in accordance with these docs.   Most of the time, you should create a key pair with encryption enabled for the private key.</li> <li>Add the private key to the CalData 1Password vault, along with the intended service account user name and passphrase (if applicable)</li> <li>Create a new user in the Snowflake Terraform configuration (<code>users.tf</code>) and assign it the appropriate functional role.   Once the user is created, add its public key in the Snowflake UI:   <pre><code>ALTER USER &lt;USERNAME&gt; SET RSA_PUBLIC_KEY='MII...'\n</code></pre>   Note that we need to remove the header and trailer (i.e. <code>-- BEGIN PUBLIC KEY --</code>) as well as any line breaks   in order for Snowflake to accept the public key as valid.</li> <li>Add the private key for the user to whatever system needs to access Snowflake.</li> </ol> <p>Service accounts should not be shared across different applications, so if one becomes compromised, the damage is more isolated.</p>"},{"location":"setup/snowflake-setup/","title":"New project setup","text":"<p>The DSE team regularly creates new Snowflake accounts in our Snowflake org. We do this instead of putting all of our data projects into our main account for a few reasons:</p> <ol> <li>At the end of a project, we often want to transfer account ownership to our partners.     Having it separated from the start helps that process.</li> <li>We frequently want to add our project champion or IT partners to our account as admins.     This is safer if project accounts are separate.</li> <li>We often want to have accounts in a specific cloud and region for compliance or data transfer regions.</li> <li>Different projects may require different approaches to account-level operations like OAuth/SAML.</li> </ol> <p>Here we document the steps to creating a new Snowflake account from scratch.</p>"},{"location":"setup/snowflake-setup/#prerequisites","title":"Prerequisites","text":""},{"location":"setup/snowflake-setup/#obtain-permissions-in-snowflake","title":"Obtain permissions in Snowflake","text":"<p>In order to create a new account, you will need access to the <code>orgadmin</code> role. If you have <code>accountadmin</code> in the primary Snowflake account, you can grant it to yourself:</p> <pre><code>USE ROLE accountadmin;\nGRANT ROLE orgadmin TO USER &lt;YOUR-USER&gt;;\n</code></pre> <p>If you later want to revoke the <code>orgadmin</code> role from your user or any other, you can do so with:</p> <pre><code>USE ROLE accountadmin;\nREVOKE ROLE orgadmin FROM USER &lt;YOUR-USER&gt;;\n</code></pre>"},{"location":"setup/snowflake-setup/#get-access-to-aws","title":"Get access to AWS","text":"<p>We typically create our Snowflake architecture using Terraform. Terraform state is stored in S3 buckets within our AWS account, so you will need read/write access to those buckets.</p> <p>Ask a DSE AWS admin to give you access to these buckets, and configure your AWS credentials.</p>"},{"location":"setup/snowflake-setup/#install-terraform-dependencies","title":"Install terraform dependencies","text":"<p>You can install Terraform using whatever approach makes sense for your system, including using <code>brew</code> or <code>conda</code>.</p> <p>Here is a sample for installing the dependencies using <code>conda</code>:</p> <pre><code>conda create -n infra python=3.10  # create an environment named 'infra'\nconda activate infra  # activate the environment\nconda install -c conda-forge terraform tflint  # Install terraform and tflint\n</code></pre>"},{"location":"setup/snowflake-setup/#snowflake-account-setup","title":"Snowflake account setup","text":""},{"location":"setup/snowflake-setup/#create-the-account","title":"Create the account","text":"<ol> <li>Assume the <code>ORGADMIN</code> role</li> <li>Under the \"Admin\" side panel, go to \"Accounts\" and click the \"+ Account\" button:<ol> <li>Select the cloud and region appropriate to the project. The region should be in the United States.</li> <li>Select \"Business Critical\" for the Snowflake Edition.</li> <li>You will be prompted to create an initial user with <code>ACCOUNTADMIN</code> privileges. This should be you.     You will be prompted to create a password for your user. Create one using your password manager,     but know that it will ask you to change your password upon first log-in.</li> <li>Save the Account Locator, Organization name, Account name and Account URL for your new account.</li> </ol> </li> <li>Log into your new account. You should be prompted to change your password. Save the updated password in your password manager.</li> </ol>"},{"location":"setup/snowflake-setup/#enable-multi-factor-authentication-for-your-user","title":"Enable multi-factor authentication for your user","text":"<ol> <li>Ensure the Duo Mobile app is installed on your phone.</li> <li>In the upper-left corner of the Snowsight UI, click on your username, and select \"Profile\"</li> <li>At the bottom of the dialog, select \"Enroll\" to enable multi-factor authentication.</li> <li>Follow the instructions to link the new account with your Duo app.</li> </ol>"},{"location":"setup/snowflake-setup/#set-up-key-pair-authentication","title":"Set up key pair authentication","text":"<p>Certain Snowflake clients don't properly cache MFA tokens, which means that using them can generate dozens or hundreds of MFA requests on your phone. At best this makes the tools unusable, and at worst it can lock your Snowflake account. One example of such a tool is (as of this writing) the Snowflake Terraform Provider.</p> <p>The recommended workaround for this is to add a key pair to your account for use with those tools.</p> <ol> <li>Follow the instructions given here     to generate a key pair and add the public key to your account.     Keep the key pair in a secure place on your device.     This gist     contains the bash commands from the instructions,     and can be helpful for quickly creating a new encrypted key pair.     Usage of the script looks like:     <pre><code>bash generate_encrypted_key.sh &lt;key-name&gt; &lt;passphrase&gt;\n</code></pre>     You can use <code>pbcopy &lt; _your_public_key_file_name_.pub</code> to copy the contents of your public key.     Be sure to remove the <code>----BEGIN PUBLIC KEY----</code> and <code>-----END PUBLIC KEY------</code> portions     when adding your key to your Snowflake user.</li> <li>In your local <code>.bash_profile</code> or an <code>.env</code> file, add environment variables for     <code>SNOWFLAKE_ACCOUNT</code>, <code>SNOWFLAKE_USER</code>, <code>SNOWFLAKE_PRIVATE_KEY_PATH</code>,     and (if applicable) <code>SNOWFLAKE_PRIVATE_KEY_PASSPHRASE</code>.</li> </ol>"},{"location":"setup/snowflake-setup/#apply-a-session-policy","title":"Apply a session policy","text":"<p>By default, Snowflake logs out user sessions after four hours of inactivity. ODI's information security policies prefer that we log out after one hour of inactivity for most accounts, and after fifteen minutes of inactivity for particularly sensitive accounts.</p> <p>Note</p> <p>It's possible we will do this using Terraform in the future, but at the time of this writing the Snowflake Terraform provider does not support session policies.</p> <p>After the Snowflake account is created, run the following script in a worksheet to set the appropriate session policy:</p> <pre><code>use role sysadmin;\ncreate database if not exists policies;\ncreate session policy if not exists policies.public.account_session_policy\n  session_idle_timeout_mins = 60\n  session_ui_idle_timeout_mins = 60\n;\nuse role accountadmin;\n-- alter account unset session policy;  -- unset any previously existing session policy\nalter account set session policy policies.public.account_session_policy;\n</code></pre>"},{"location":"setup/snowflake-setup/#developing-against-production-data","title":"Developing against production data","text":"<p>Our Snowflake architecture allows for reasonably safe <code>SELECT</code>ing from the production databases while developing models. While this could be expensive for large tables, it also allows for faster and more reliable model development.</p> <p>To develop against production data, first you need someone with the <code>USERADMIN</code> role to grant rights to the <code>TRANSFORMER_DEV</code> role (this need only be done once, and can be revoked later). These grants enable access to real data for development and facilitate cloning and deferral for large-table projects:</p> <pre><code>USE ROLE USERADMIN;\nGRANT ROLE RAW_PRD_READ TO ROLE TRANSFORMER_DEV;\nGRANT ROLE TRANSFORM_PRD_READ TO ROLE TRANSFORMER_DEV;\nGRANT ROLE ANALYTICS_PRD_READ TO ROLE TRANSFORMER_DEV;\n</code></pre> <p>Note</p> <p>These grants are not managed via Terraform in order to keep the configurations of different environments as logically separate as possible. We may revisit this decision should the manual grants cause problems.</p> <p>You can then run dbt locally and specify the <code>RAW</code> database manually:</p> <pre><code>DBT_RAW_DB=RAW_PRD dbt run\n</code></pre>"},{"location":"setup/snowflake-setup/#streamlit-dashboard-development","title":"Streamlit dashboard development","text":"<p>All production Streamlit dashboards and their marts should reside in the <code>ANALYTICS_{env}_PRD</code> database. If a dashboard needs access to objects from earlier layers, they should be exposed via explicitly created mart tables in this database.</p> <p>To support Streamlit development, the <code>REPORTER_DEV</code> role may need read access to the production marts:</p> <pre><code>USE ROLE USERADMIN;\nGRANT ROLE ANALYTICS_PRD_READ TO ROLE REPORTER_DEV;\n</code></pre>"},{"location":"setup/snowflake-setup/#add-it-ops-representatives","title":"Add IT-Ops representatives","text":"<p>TODO: establish and document processes here.</p>"},{"location":"setup/snowflake-setup/#set-up-okta-sso-and-scim","title":"Set up Okta SSO and SCIM","text":"<p>TODO: establish and document processes here.</p>"},{"location":"setup/terraform-project-setup/","title":"Deploy project infrastructure using Terraform","text":"<p>We will create two separate deployments of the project infrastructure, one for development, and one for production. In some places we will refer to project name and owner as <code>&lt;project&gt;</code> and <code>&lt;owner&gt;</code>, respectively, following our naming conventions. You should substitute the appropriate names there.</p>"},{"location":"setup/terraform-project-setup/#create-the-dev-configuration","title":"Create the dev configuration","text":"<ol> <li>Ensure that your environment has environment variables set for     <code>SNOWFLAKE_ACCOUNT</code>, <code>SNOWFLAKE_USER</code>, <code>SNOWFLAKE_PRIVATE_KEY_PATH</code>, and <code>SNOWFLAKE_PRIVATE_KEY_PASSPHRASE</code>.     Make sure you don't have any other <code>SNOWFLAKE_*</code> variables set,     as they can interfere with authentication.</li> <li>In the new git repository, create a directory to hold the development Terraform configuration:     <pre><code>mkdir -p terraform/environments/dev/\n</code></pre>     The location of this directory is by convention, and subject to change.</li> <li>Copy the terraform configuration from     here     to your <code>dev</code> directory.</li> <li>In the \"elt\" module of <code>main.tf</code>, change the <code>source</code> parameter to point to     <code>\"github.com/cagov/data-infrastructure.git//terraform/snowflake/modules/elt?ref=&lt;ref&gt;\"</code>     where <code>&lt;ref&gt;</code> is the short hash of the most recent commit in the <code>data-infrastructure</code> repository.</li> <li>In the <code>dev</code> directory, create a new backend configuration file called <code>&lt;owner&gt;-&lt;project&gt;-dev.tfbackend</code>.     The file will point to the S3 bucket in which we are storing terraform state. Populate the backend     configuration file with the following (making sure to substitute values for <code>&lt;owner&gt;</code> and <code>&lt;project&gt;</code>):     <pre><code>bucket = \"dse-snowflake-dev-terraform-state\"\ndynamodb_table = \"dse-snowflake-dev-terraform-state-lock\"\nkey = \"&lt;owner&gt;-&lt;project&gt;-dev.tfstate\"\nregion = \"us-west-2\"\n</code></pre></li> <li>In the <code>dev</code> directory, create a terraform variables file called <code>terraform.tfvars</code>,     and populate the \"elt\" module variables. These variables may expand in the future,     but at the moment they are just the new Snowflake organization name, account name and the environment     (in this case <code>\"DEV\"</code>):     <pre><code>organization_name = \"&lt;organization_name&gt;\"\naccount_name = \"&lt;account_name&gt;\"\nenvironment = \"DEV\"\n</code></pre></li> <li>Initialize the configuration:     <pre><code>terraform init -backend-config &lt;owner&gt;-&lt;project&gt;-dev.tfbackend\n</code></pre></li> <li>Include both Mac and Linux provider binaries in your terraform lock file.     This helps mitigate differences between CI environments and ODI Macs:     <pre><code>terraform providers lock -platform=linux_amd64 -platform=darwin_amd64\n</code></pre></li> <li>Add your new <code>main.tf</code>, <code>terraform.tfvars</code>, <code>&lt;owner&gt;-&lt;project&gt;-dev.tfbackend</code>,     and terraform lock file to the git repository. Do not add the <code>.terraform/</code> directory.</li> </ol>"},{"location":"setup/terraform-project-setup/#deploy-the-dev-configuration","title":"Deploy the dev configuration","text":"<ol> <li>Ensure that your local environment has environment variables set for <code>SNOWFLAKE_ACCOUNT</code>,     <code>SNOWFLAKE_USER</code>, <code>SNOWFLAKE_PRIVATE_KEY_PATH</code>,  and <code>SNOWFLAKE_PRIVATE_KEY_PASSPHRASE</code>,     and that they are set to your new account, rather than any other accounts.</li> <li>Run <code>terraform plan</code> to see the plan for the resources that will be created.     Inspect the plan to see that everything looks correct.</li> <li>Run <code>terraform apply</code> to deploy the configuration. This will actually create the infrastructure!</li> </ol>"},{"location":"setup/terraform-project-setup/#configure-and-deploy-the-production-configuration","title":"Configure and deploy the production configuration","text":"<p>Re-run all of the steps above, but in a new directory <code>terraform/environments/prd</code>. Everywhere where there is a <code>dev</code> (or <code>DEV</code>), replace it with a <code>prd</code> (or <code>PRD</code>).</p>"}]}